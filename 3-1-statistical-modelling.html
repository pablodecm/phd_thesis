<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-71094563-2"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-71094563-2');
    </script><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Statistical Learning and Inference at Particle Collider Experiments</title><meta name="description" content="Statistical Learning and Inference at Particle Collider Experiments"><meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7"><meta property="og:title" content="Statistical Learning and Inference at Particle Collider Experiments"><meta property="og:type" content="book"><meta name="github-repo" content="pablodecm/phd_thesis"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Statistical Learning and Inference at Particle Collider Experiments"><meta name="author" content="Pablo de Castro Manzano"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><script src="libs/jquery/jquery.min.js"></script><link href="libs/gitbook/css/style.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-table.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-search.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet"><link href="css/style.css" rel="stylesheet"><link href="css/toc.css" rel="stylesheet"></head><body>

    
    <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

      <div class="book-summary">
        <nav role="navigation"><ul class="summary"><li>
              <a href="./">PhD Thesis - Pablo de Castro</a>
            </li>
            <li class="divider">
            <li class="chapter" data-level="" data-path="abstract.html">
              <a href="abstract.html"><i class="fa fa-check"></i> Abstract</a>
            </li>
            <li class="chapter" data-level="" data-path="preface.html">
              <a href="preface.html"><i class="fa fa-check"></i> Preface</a>
            </li>
            <li class="chapter" data-level="" data-path="acknowledgements.html">
              <a href="acknowledgements.html"><i class="fa fa-check"></i> Acknowledgements</a>
            </li>
            <li class="chapter" data-level="" data-path="introduction.html">
              <a href="introduction.html"><i class="fa fa-check"></i> Introduction</a>
            </li>
            <li class="chapter" data-level="1" data-path="1-theory-of-fundamental-interactions.html">
              <a href="1-theory-of-fundamental-interactions.html"><i class="fa fa-check"></i><b>1</b> Theory of Fundamental Interactions</a>
              <ul><li class="chapter" data-level="1.1" data-path="1-1-the-standard-model.html">
                  <a href="1-1-the-standard-model.html"><i class="fa fa-check"></i><b>1.1</b> The Standard Model</a>
                  <ul><li class="chapter" data-level="1.1.1" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:qft_basics"><i class="fa fa-check"></i><b>1.1.1</b> Essentials of Quantum Field Theory</a>
                    </li>
                    <li class="chapter" data-level="1.1.2" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:qcd_detail"><i class="fa fa-check"></i><b>1.1.2</b> Quantum Chromodynamics</a>
                    </li>
                    <li class="chapter" data-level="1.1.3" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:ew_detail"><i class="fa fa-check"></i><b>1.1.3</b> Electroweak Interactions</a>
                    </li>
                    <li class="chapter" data-level="1.1.4" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:ewsb_higgs"><i class="fa fa-check"></i><b>1.1.4</b> Symmetry Breaking and the Higgs Boson</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="1.2" data-path="1-2-beyond-the-standard-model.html">
                  <a href="1-2-beyond-the-standard-model.html"><i class="fa fa-check"></i><b>1.2</b> Beyond the Standard Model</a>
                  <ul><li class="chapter" data-level="1.2.1" data-path="1-2-beyond-the-standard-model.html">
                      <a href="1-2-beyond-the-standard-model.html#known-limitations"><i class="fa fa-check"></i><b>1.2.1</b> Known Limitations</a>
                    </li>
                    <li class="chapter" data-level="1.2.2" data-path="1-2-beyond-the-standard-model.html">
                      <a href="1-2-beyond-the-standard-model.html#sec:possible_ext"><i class="fa fa-check"></i><b>1.2.2</b> Possible Extensions</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="1.3" data-path="1-3-phenomenology-of-proton-collisions.html">
                  <a href="1-3-phenomenology-of-proton-collisions.html"><i class="fa fa-check"></i><b>1.3</b> Phenomenology of Proton Collisions</a>
                  <ul><li class="chapter" data-level="1.3.1" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:main_obs"><i class="fa fa-check"></i><b>1.3.1</b> Main Observables</a>
                    </li>
                    <li class="chapter" data-level="1.3.2" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:pdfs"><i class="fa fa-check"></i><b>1.3.2</b> Parton Distribution Functions</a>
                    </li>
                    <li class="chapter" data-level="1.3.3" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:factorisation"><i class="fa fa-check"></i><b>1.3.3</b> Factorisation and Generation of Hard Processes</a>
                    </li>
                    <li class="chapter" data-level="1.3.4" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:parton_showers"><i class="fa fa-check"></i><b>1.3.4</b> Hadronization and Parton Showers</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="2" data-path="2-experiments-at-particle-colliders.html">
              <a href="2-experiments-at-particle-colliders.html"><i class="fa fa-check"></i><b>2</b> Experiments at Particle Colliders</a>
              <ul><li class="chapter" data-level="2.1" data-path="2-1-the-large-hadron-collider.html">
                  <a href="2-1-the-large-hadron-collider.html"><i class="fa fa-check"></i><b>2.1</b> The Large Hadron Collider</a>
                  <ul><li class="chapter" data-level="2.1.1" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#injection-and-acceleration-chain"><i class="fa fa-check"></i><b>2.1.1</b> Injection and Acceleration Chain</a>
                    </li>
                    <li class="chapter" data-level="2.1.2" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#sec:op_pars"><i class="fa fa-check"></i><b>2.1.2</b> Operation Parameters</a>
                    </li>
                    <li class="chapter" data-level="2.1.3" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#sec:pile_up"><i class="fa fa-check"></i><b>2.1.3</b> Multiple Hadron Interactions</a>
                    </li>
                    <li class="chapter" data-level="2.1.4" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#sec:lhc_experiments"><i class="fa fa-check"></i><b>2.1.4</b> Experiments</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="2.2" data-path="2-2-the-compact-muon-solenoid.html">
                  <a href="2-2-the-compact-muon-solenoid.html"><i class="fa fa-check"></i><b>2.2</b> The Compact Muon Solenoid</a>
                  <ul><li class="chapter" data-level="2.2.1" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:exp_geom"><i class="fa fa-check"></i><b>2.2.1</b> Experimental Geometry</a>
                    </li>
                    <li class="chapter" data-level="2.2.2" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_magnet"><i class="fa fa-check"></i><b>2.2.2</b> Magnet</a>
                    </li>
                    <li class="chapter" data-level="2.2.3" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_tracking"><i class="fa fa-check"></i><b>2.2.3</b> Tracking System</a>
                    </li>
                    <li class="chapter" data-level="2.2.4" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_ecal"><i class="fa fa-check"></i><b>2.2.4</b> Electromagnetic Calorimeter</a>
                    </li>
                    <li class="chapter" data-level="2.2.5" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_hcal"><i class="fa fa-check"></i><b>2.2.5</b> Hadronic Calorimeter</a>
                    </li>
                    <li class="chapter" data-level="2.2.6" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_muon"><i class="fa fa-check"></i><b>2.2.6</b> Muon System</a>
                    </li>
                    <li class="chapter" data-level="2.2.7" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:trigger"><i class="fa fa-check"></i><b>2.2.7</b> Trigger and Data Acquisition</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="2.3" data-path="2-3-event-simulation-and-reconstruction.html">
                  <a href="2-3-event-simulation-and-reconstruction.html"><i class="fa fa-check"></i><b>2.3</b> Event Simulation and Reconstruction</a>
                  <ul><li class="chapter" data-level="2.3.1" data-path="2-3-event-simulation-and-reconstruction.html">
                      <a href="2-3-event-simulation-and-reconstruction.html#sec:gen_view"><i class="fa fa-check"></i><b>2.3.1</b> A Generative View</a>
                    </li>
                    <li class="chapter" data-level="2.3.2" data-path="2-3-event-simulation-and-reconstruction.html">
                      <a href="2-3-event-simulation-and-reconstruction.html#sec:detector_simulation"><i class="fa fa-check"></i><b>2.3.2</b> Detector Simulation</a>
                    </li>
                    <li class="chapter" data-level="2.3.3" data-path="2-3-event-simulation-and-reconstruction.html">
                      <a href="2-3-event-simulation-and-reconstruction.html#sec:event_reco"><i class="fa fa-check"></i><b>2.3.3</b> Event Reconstruction</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="3" data-path="3-statistical-modelling-and-inference-at-the-lhc.html">
              <a href="3-statistical-modelling-and-inference-at-the-lhc.html"><i class="fa fa-check"></i><b>3</b> Statistical Modelling and Inference at the LHC</a>
              <ul><li class="chapter" data-level="3.1" data-path="3-1-statistical-modelling.html">
                  <a href="3-1-statistical-modelling.html"><i class="fa fa-check"></i><b>3.1</b> Statistical Modelling</a>
                  <ul><li class="chapter" data-level="3.1.1" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#sec:model_overview"><i class="fa fa-check"></i><b>3.1.1</b> Overview</a>
                    </li>
                    <li class="chapter" data-level="3.1.2" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#simulation-as-generative-modelling"><i class="fa fa-check"></i><b>3.1.2</b> Simulation as Generative Modelling</a>
                    </li>
                    <li class="chapter" data-level="3.1.3" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#sec:dim_reduction"><i class="fa fa-check"></i><b>3.1.3</b> Dimensionality Reduction</a>
                    </li>
                    <li class="chapter" data-level="3.1.4" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#sec:known_unknowns"><i class="fa fa-check"></i><b>3.1.4</b> Known Unknowns</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="3.2" data-path="3-2-statistical-inference.html">
                  <a href="3-2-statistical-inference.html"><i class="fa fa-check"></i><b>3.2</b> Statistical Inference</a>
                  <ul><li class="chapter" data-level="3.2.1" data-path="3-2-statistical-inference.html">
                      <a href="3-2-statistical-inference.html#sec:likelihood-free"><i class="fa fa-check"></i><b>3.2.1</b> Likelihood-Free Inference</a>
                    </li>
                    <li class="chapter" data-level="3.2.2" data-path="3-2-statistical-inference.html">
                      <a href="3-2-statistical-inference.html#sec:hypo_test"><i class="fa fa-check"></i><b>3.2.2</b> Hypothesis Testing</a>
                    </li>
                    <li class="chapter" data-level="3.2.3" data-path="3-2-statistical-inference.html">
                      <a href="3-2-statistical-inference.html#sec:param_est"><i class="fa fa-check"></i><b>3.2.3</b> Parameter Estimation</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="4" data-path="4-machine-learning-in-high-energy-physics.html">
              <a href="4-machine-learning-in-high-energy-physics.html"><i class="fa fa-check"></i><b>4</b> Machine Learning in High-Energy Physics</a>
              <ul><li class="chapter" data-level="4.1" data-path="4-1-problem-description.html">
                  <a href="4-1-problem-description.html"><i class="fa fa-check"></i><b>4.1</b> Problem Description</a>
                  <ul><li class="chapter" data-level="4.1.1" data-path="4-1-problem-description.html">
                      <a href="4-1-problem-description.html#sec:supervised"><i class="fa fa-check"></i><b>4.1.1</b> Probabilistic Classification and Regression</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="4.2" data-path="4-2-machine-learning-techniques.html">
                  <a href="4-2-machine-learning-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Machine Learning Techniques</a>
                  <ul><li class="chapter" data-level="4.2.1" data-path="4-2-machine-learning-techniques.html">
                      <a href="4-2-machine-learning-techniques.html#sec:boosted_decision_trees"><i class="fa fa-check"></i><b>4.2.1</b> Boosted Decision Trees</a>
                    </li>
                    <li class="chapter" data-level="4.2.2" data-path="4-2-machine-learning-techniques.html">
                      <a href="4-2-machine-learning-techniques.html#sec:ann"><i class="fa fa-check"></i><b>4.2.2</b> Artificial Neural Networks</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="4.3" data-path="4-3-applications-in-high-energy-physics.html">
                  <a href="4-3-applications-in-high-energy-physics.html"><i class="fa fa-check"></i><b>4.3</b> Applications in High Energy Physics</a>
                  <ul><li class="chapter" data-level="4.3.1" data-path="4-3-applications-in-high-energy-physics.html">
                      <a href="4-3-applications-in-high-energy-physics.html#sec:sig_vs_bkg"><i class="fa fa-check"></i><b>4.3.1</b> Signal vs Background Classification</a>
                    </li>
                    <li class="chapter" data-level="4.3.2" data-path="4-3-applications-in-high-energy-physics.html">
                      <a href="4-3-applications-in-high-energy-physics.html#sec:particle_id_reg"><i class="fa fa-check"></i><b>4.3.2</b> Particle Identification and Regression</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="5" data-path="5-search-for-anomalous-higgs-pair-production-with-cms.html">
              <a href="5-search-for-anomalous-higgs-pair-production-with-cms.html"><i class="fa fa-check"></i><b>5</b> Search for Anomalous Higgs Pair Production with CMS</a>
              <ul><li class="chapter" data-level="5.1" data-path="5-1-introduction.html">
                  <a href="5-1-introduction.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
                </li>
                <li class="chapter" data-level="5.2" data-path="5-2-higgs-pair-production-and-anomalous-couplings.html">
                  <a href="5-2-higgs-pair-production-and-anomalous-couplings.html"><i class="fa fa-check"></i><b>5.2</b> Higgs Pair Production and Anomalous Couplings</a>
                </li>
                <li class="chapter" data-level="5.3" data-path="5-3-analysis-strategy.html">
                  <a href="5-3-analysis-strategy.html"><i class="fa fa-check"></i><b>5.3</b> Analysis Strategy</a>
                </li>
                <li class="chapter" data-level="5.4" data-path="5-4-trigger-and-datasets.html">
                  <a href="5-4-trigger-and-datasets.html"><i class="fa fa-check"></i><b>5.4</b> Trigger and Datasets</a>
                </li>
                <li class="chapter" data-level="5.5" data-path="5-5-event-selection.html">
                  <a href="5-5-event-selection.html"><i class="fa fa-check"></i><b>5.5</b> Event Selection</a>
                </li>
                <li class="chapter" data-level="5.6" data-path="5-6-data-driven-background-estimation.html">
                  <a href="5-6-data-driven-background-estimation.html"><i class="fa fa-check"></i><b>5.6</b> Data-Driven Background Estimation</a>
                  <ul><li class="chapter" data-level="5.6.1" data-path="5-6-data-driven-background-estimation.html">
                      <a href="5-6-data-driven-background-estimation.html#sec:hem_mixing"><i class="fa fa-check"></i><b>5.6.1</b> Hemisphere Mixing</a>
                    </li>
                    <li class="chapter" data-level="5.6.2" data-path="5-6-data-driven-background-estimation.html">
                      <a href="5-6-data-driven-background-estimation.html#sec:bkg_validation"><i class="fa fa-check"></i><b>5.6.2</b> Background Validation</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="5.7" data-path="5-7-systematic-uncertainties.html">
                  <a href="5-7-systematic-uncertainties.html"><i class="fa fa-check"></i><b>5.7</b> Systematic Uncertainties</a>
                </li>
                <li class="chapter" data-level="5.8" data-path="5-8-analysis-results.html">
                  <a href="5-8-analysis-results.html"><i class="fa fa-check"></i><b>5.8</b> Analysis Results</a>
                </li>
                <li class="chapter" data-level="5.9" data-path="5-9-combination-with-other-decay-channels.html">
                  <a href="5-9-combination-with-other-decay-channels.html"><i class="fa fa-check"></i><b>5.9</b> Combination with Other Decay Channels</a>
                </li>
              </ul></li>
            <li class="chapter" data-level="6" data-path="6-inference-aware-neural-optimisation.html">
              <a href="6-inference-aware-neural-optimisation.html"><i class="fa fa-check"></i><b>6</b> Inference-Aware Neural Optimisation</a>
              <ul><li class="chapter" data-level="6.1" data-path="6-1-introduction.html">
                  <a href="6-1-introduction.html"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
                </li>
                <li class="chapter" data-level="6.2" data-path="6-2-problem-statement.html">
                  <a href="6-2-problem-statement.html"><i class="fa fa-check"></i><b>6.2</b> Problem Statement</a>
                </li>
                <li class="chapter" data-level="6.3" data-path="6-3-method.html">
                  <a href="6-3-method.html"><i class="fa fa-check"></i><b>6.3</b> Method</a>
                </li>
                <li class="chapter" data-level="6.4" data-path="6-4-related-work.html">
                  <a href="6-4-related-work.html"><i class="fa fa-check"></i><b>6.4</b> Related Work</a>
                </li>
                <li class="chapter" data-level="6.5" data-path="6-5-experiments.html">
                  <a href="6-5-experiments.html"><i class="fa fa-check"></i><b>6.5</b> Experiments</a>
                  <ul><li class="chapter" data-level="6.5.1" data-path="6-5-experiments.html">
                      <a href="6-5-experiments.html#sec:synthetic_mixture"><i class="fa fa-check"></i><b>6.5.1</b> 3D Synthetic Mixture</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="7" data-path="7-conclusions-and-prospects.html">
              <a href="7-conclusions-and-prospects.html"><i class="fa fa-check"></i><b>7</b> Conclusions and Prospects</a>
            </li>
            <li class="chapter" data-level="" data-path="references.html">
              <a href="references.html"><i class="fa fa-check"></i> References</a>
            </li>
          </ul></nav></div>

      <div class="book-body">
        <div class="body-inner">
          <div class="book-header" role="navigation">
            <h1>
              <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Inference at Particle Collider Experiments</a>
            </h1>
          </div>

          <div class="page-wrapper" tabindex="-1" role="main">
            <div class="page-inner">

              <section class="normal" id="section-"><div id="sec:stat_model" class="section level2">
                <h2><span class="header-section-number">3.1</span> Statistical Modelling</h2>
                <p>An essential element for carrying out statistical inference is the availability of an adequate statistical model. In this section, the main characteristics of the statistical models used in particle collider analyses will be formally developed from first principles. This methodology allows a mathematical approach to their structure and factorisation. This will prove useful to establish a formal link between the techniques discussed in the next chapters and the simulation-based generative models that are often used to describe the data. Additionally, the role and importance of event selection, event reconstruction and dimensionality reduction - i.e. the compression of the relevant information from high-dimensional data into a lower-dimensional representation, such as the output of a multivariate classifier - will be described in the larger statistical framework of an LHC analysis. Lastly, the main approaches commonly followed to construct synthetic<a href="3-1-statistical-modelling.html#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> likelihoods that efficiently connect summaries of the detector observation with the parameters of interest will be illustrated.</p>
                <div id="sec:model_overview" class="section level3">
                <h3><span class="header-section-number">3.1.1</span> Overview</h3>
                <p>Let us suppose that we record a collection of raw detector readouts <span class="math inline">\(D = \{\boldsymbol{x}_0,...,\boldsymbol{x}_n\}\)</span> for a total of <span class="math inline">\(n\)</span> bunch crossings at a particle collider experiment, such as CMS at the LHC (see Section <a href="2-2-the-compact-muon-solenoid.html#sec:cms">2.2</a>). Note that vector notation is used for each individual readout, also referred to as event, because for mathematical simplification we will be assuming that each detector observation can be embedded - in the mathematical sense - as a member of a fixed size <span class="math inline">\(d\)</span>-dimensional space, i.e. <span class="math inline">\(\boldsymbol{x} \in \mathcal{X} \subseteq \mathbb{R}^d\)</span>, even though variable-size sets or tree-like structures might be a more compact and useful representation in practice, as will be discussed later. As a starting point, let us assume for simplicity that the detector readout for every bunch crossing is recorded, i.e. no trigger filtering system as the one described in Section <a href="2-2-the-compact-muon-solenoid.html#sec:trigger">2.2.7</a> is in place, hence after each bunch crossing <span class="math inline">\(i\)</span> a given raw detector readout <span class="math inline">\(\boldsymbol{x}_i\)</span> will be obtained. From here onwards, each event/observation/readout will be assumed to be independent and identically distributed (i.i.d.), a reasonable approximation if the experimental conditions are stable during the acquisition period as discussed at the beginning of Section <a href="2-3-event-simulation-and-reconstruction.html#sec:event">2.3</a>; consequently the event ordering or index <span class="math inline">\(i\)</span> are not relevant.</p>
                <div id="experiment-outcome" class="section level4">
                <h4><span class="header-section-number">3.1.1.1</span> Experiment Outcome</h4>
                <p>Within the above framework, we could begin by posing the question of how we expect the readout output, which can be effectively treated as a random variable <span class="math inline">\(\boldsymbol{x}\)</span>, to be distributed and how such distribution is related with the (theoretical) parameters we are interested in measuring or the model extensions we are interested in testing using the experiment. We would like then to model the probability density distribution function generating a given observation <span class="math inline">\(\boldsymbol{x}_i\)</span> conditional on the parameters of interest, that is: <span id="eq:cond_density"><span class="math display">\[
                  \boldsymbol{x}_i \sim p ( \boldsymbol{x}|\boldsymbol{\theta} )
                \qquad(3.1)\]</span></span> where <span class="math inline">\(\boldsymbol{\theta} \in \mathcal{\Theta} \subseteq \mathbb{R}^p\)</span> denotes all the parameters we are interested in and affects the detector outcome of collisions. As will be extensively discussed in this chapter, an analytical or even tractable approximation of <span class="math inline">\(p ( \boldsymbol{x}|\boldsymbol{\theta})\)</span> is not attainable, given that we are considering <span class="math inline">\(\boldsymbol{x}\)</span> to be a representation of the raw readout of all sub-detectors, thus its dimensionality <span class="math inline">\(d\)</span> can be of the order <span class="math inline">\(\mathcal{O}(10^8)\)</span>. It is worth mentioning that even <span class="math inline">\(d\)</span> is very high, each observation is usually extremely sparse given that most of the detectors would not sense any signal. The total number of observations <span class="math inline">\(n\)</span> is also very large at modern colliders, e.g. a collision occurs each <span class="math inline">\(25\ \textrm{ns}\)</span> at the LHC. Furthermore, the known interactions that produce the set of particles of the event as well as the subsequent physical processes that generate the readouts in the detectors are overly complex, and realistic modelling can only be obtained through simulation, as jointly reviewed in Section <a href="1-3-phenomenology-of-proton-collisions.html#sec:pheno">1.3</a> and Section <a href="2-3-event-simulation-and-reconstruction.html#sec:event">2.3</a>.</p>
                </div>
                <div id="mixture-structure" class="section level4">
                <h4><span class="header-section-number">3.1.1.2</span> Mixture Structure</h4>
                <p>While a detailed closed-form description of <span class="math inline">\(p(\boldsymbol{x}|\boldsymbol{\theta})\)</span> cannot be obtained, we can safely make a very useful remark about its basic structure, which is fundamental for simplyfing the statistical treatment of particle collider observations and simulations, and was already hinted at in Section <a href="1-3-phenomenology-of-proton-collisions.html#sec:main_obs">1.3.1</a> when discussing the possible outcomes of fundamental proton-proton interactions. The underlying process generating <span class="math inline">\(\boldsymbol{x}\)</span> can be treated as a <em>mixture model</em>, which can be expressed as the probabilistic composition of samples from multiple probabilistic distributions corresponding to different types of interaction processes occurring in the collision. If we knew the probabilistic distribution function of each mixture component <span class="math inline">\(p_j(\boldsymbol{x}|\boldsymbol{\theta})\)</span> then <span class="math inline">\(p ( \boldsymbol{x}|\boldsymbol{\theta} )\)</span> could be expressed as: <span id="eq:mixture_pdf"><span class="math display">\[
                p ( \boldsymbol{x}|\boldsymbol{\theta} ) =
                \sum^{K-1}_{j=0} \phi_j \ p_j ( \boldsymbol{x}|\boldsymbol{\theta} )
                \qquad(3.2)\]</span></span> where <span class="math inline">\(K\)</span> is the number of mixture components and <span class="math inline">\(\phi_j\)</span> is the mixture weight/fraction, i.e. probability for a sample to be originated from each mixture component <span class="math inline">\(j\)</span>. The specifics of the mixture expansion as well as the total number of mixture components are not uniquely defined, but are based on the independence of groups of physical processes, as will be discussed later. Practically, each <span class="math inline">\(p_j(\boldsymbol{x}|\boldsymbol{\theta})\)</span> will be intractable due to the exact same reason that <span class="math inline">\(p ( \boldsymbol{x}|\boldsymbol{\theta} )\)</span> is intractable, thus a more sensible description of the mixture model is its generative definition, described by the following two-step sampling procedure: <span id="eq:mixture_gen"><span class="math display">\[ z_i \sim \textrm{Categorical}(\boldsymbol{\phi})
                \quad \longrightarrow  \quad
                \boldsymbol{x}_i \sim p_{z_i}( \boldsymbol{x} | \boldsymbol{\theta})
                \qquad(3.3)\]</span></span> describing the sampling of random integer <span class="math inline">\(z_i \in \{0, \dots, K -1 \}\)</span> from a random categorical<a href="3-1-statistical-modelling.html#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> distribution and the subsequent sampling of the corresponding mixture component indexed by <span class="math inline">\(z_i\)</span>, where <span class="math inline">\(\boldsymbol{\phi} = \{\phi_0, \dots, \phi_{K-1} \}\)</span> is the vector of probabilities for each of the mixture components.  For here onwards, mixture models might in some cases be portrayed by using the analytical depiction as in Equation <a href="3-1-statistical-modelling.html#eq:mixture_pdf">3.2</a>, always noting that the generative approach might be more convenient for the actual estimation of expectation values when the mixture component distributions <span class="math inline">\(p_j(\boldsymbol{x}|\boldsymbol{\theta})\)</span> are not tractable.</p>
                </div>
                <div id="sec:mixture_components" class="section level4">
                <h4><span class="header-section-number">3.1.1.3</span> Mixture Components</h4>
                <p>The mixture model structure can be directly linked to the physical processes happening in fundamental proton-proton collisions and within the detectors used to study them, as described in previous chapters. As an additional simplification for now, let us neglect the effect of multiple particle interactions, described in Section <a href="2-1-the-large-hadron-collider.html#sec:pile_up">2.1.3</a>. For each proton bunch crossing, hard interactions (i.e. ones associated with a large characteristic energy scale <span class="math inline">\(Q^2\)</span>, whose cut-off does not have be specified for this particular argument) between partons might or might not occur, given the stochastic nature of the scattering processes. We could nevertheless associate a probability for a hard interaction happening <span class="math inline">\(\phi_{\textrm{hard}}\)</span>, as well to it not happening <span class="math inline">\(\phi_{\textrm{not-hard}} = 1-\phi_{\textrm{hard}}\)</span>. Given the proton colliding conditions at the LHC, the latter case is much more likely, i.e. <span class="math inline">\(\phi_{\textrm{not-hard}} \gg \phi_{\textrm{hard}}\)</span>, yet the relative probabilities depend on the energy scale cut-off considered.</p>
                <p>We can further break each previously mentioned category in sub-components corresponding to different types of processes. The hard interaction category can itself be expressed as a mixture of groups of physical interactions that can produce a hard scattering<a href="3-1-statistical-modelling.html#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>, so the probability <span class="math inline">\(\phi_{\textrm{hard}}\)</span> can be expresses as the following sum: <span id="eq:hard_prob"><span class="math display">\[ \phi_{\textrm{hard}} = \phi_0 + \dots + \phi_{K-2} = \sum_{k \in H} \phi_k
                \qquad(3.4)\]</span></span> where <span class="math inline">\(H\)</span> represents a given set of independent contributions <span class="math inline">\(k\)</span>, each characterised by a distribution <span class="math inline">\(p_j(\boldsymbol{x}|\boldsymbol{\theta})\)</span>, which depends on the group <span class="math inline">\(j\)</span> of processes that produce hard scatterings. Such a set is not uniquely defined nor its the number of elements, given that any two components <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in <span class="math inline">\(H\)</span> can be substituted by <span class="math inline">\(c\)</span>, where <span class="math inline">\(\phi_c = \phi_a + \phi_b\)</span> and <span id="eq:mixture_mixing"><span class="math display">\[p_c(\boldsymbol{x}|\boldsymbol{\theta})=
                  \frac{\phi_a}{\phi_a+\phi_b} \ p_a(\boldsymbol{x}|\boldsymbol{\theta}) +
                  \frac{\phi_b}{\phi_a+\phi_b} \ p_b(\boldsymbol{x}|\boldsymbol{\theta})
                \qquad(3.5)\]</span></span> which can be applied recursively to alter the number of components in the set. Independently on the basis chosen for the mixture expansion, in general it is not possible to infer the latent category <span class="math inline">\(z_i\)</span> (see Equation <a href="3-1-statistical-modelling.html#eq:mixture_pdf">3.2</a> given an observation <span class="math inline">\(\boldsymbol{x}_i\)</span>, because <span class="math inline">\(\boldsymbol{x}_i\)</span> may be in the support of several mixture components <span class="math inline">\(p_j(\boldsymbol{x}|\boldsymbol{\theta})\)</span>. Only probabilistic statements about the generative group <span class="math inline">\(j\)</span> can be made based on the observations.</p>
                <p>A convenient definition for the set <span class="math inline">\(H\)</span> is one that is aligned with the way theoretical calculations are carried out, given that the relative probability for a given process <span class="math inline">\(\phi_{pp\rightarrow X}\)</span> will be proportional to its total cross section <span class="math inline">\(\sigma (pp\rightarrow X)\)</span>, while its readout distribution will depend on its differential cross section <span class="math inline">\(d\sigma (pp\rightarrow X)\)</span> and its support (i.e. subset of the function domain not mapped to zero). In fact, given that the total and differential cross sections are proportional to the matrix element squared (see Section <a href="1-1-the-standard-model.html#sec:qft_basics">1.1.1</a>) of a given process <span class="math inline">\(d\sigma (pp\rightarrow X) \propto |\mathcal{M}|^2\)</span>, it is often possible to further divide each process into the cross product of Feynman diagram expansions (including interference terms). This can be a very useful notion for some analysis use cases, and is related with the approach that will be used in Chapter <a href="5-search-for-anomalous-higgs-pair-production-with-cms.html#sec:higgs_pair">5</a>.</p>
                </div>
                <div id="sec:sig_and_bkg" class="section level4">
                <h4><span class="header-section-number">3.1.1.4</span> Signal and Background</h4>
                <p>Oftentimes, we are interested in studying a subset <span class="math inline">\(S \subset H\)</span> of all the hard interaction processes, which will be referred to as signal set in what follows. This can be a single type of physical process <span class="math inline">\(\sigma (pp\rightarrow X)\)</span>, e.g. the inclusive production of a pair of Higgs bosons <span class="math inline">\(\sigma (pp\rightarrow \textrm{HH} + \textrm{other})\)</span>, or several, which it can be effectively viewed as one mixture component using Equation <a href="3-1-statistical-modelling.html#eq:mixture_mixing">3.5</a>. We can accordingly define the background subset <span class="math inline">\(B = H - S\)</span>, as the result of all other generating processes in <span class="math inline">\(H\)</span> that we are not interested in, a definition which could also be extended to include collisions where non-hard processes occurred if needed. Such distinction between generating processes of interest <span class="math inline">\(S\)</span> and background <span class="math inline">\(B\)</span> is at the root of every analysis at the LHC and it is motivated by the fact that small changes of the parameters of the SM or its theoretical extensions/alternatives affect only a subset of the produced processes at leading order, those that are governed by the interactions linked to the parameter.</p>
                <p>As a matter of a fact, customarily statistical inference at the LHC is not carried out directly on the parameters of the SM or the extension being studied, but on the relative frequency of the set of processes of interest <span class="math inline">\(\phi_S\)</span> or the properties of its distribution <span class="math inline">\(p_S(\boldsymbol{x}|\boldsymbol{\theta})\)</span>. As previously mentioned, the former is proportional to the cross section of the signal processes <span class="math inline">\(\sigma_S\)</span> (see Section <a href="1-3-phenomenology-of-proton-collisions.html#sec:pheno">1.3</a>) while the latter can include properties such as the mass of an intermediate particle resonance<a href="3-1-statistical-modelling.html#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> (e.g. the Higgs mass <span class="math inline">\(m_\textrm{H}\)</span>) or the general behaviour of the differential distribution (i.e. using unfolding methods to remove the experimental effects, which are not discussed in this work). Those parametric proxies can then be used by comparing them with the theoretical predictions of the SM or the alternative considered, in order to exclude it or constrain its fundamental parameters (i.e. those that appear in the Lagrangian).</p>
                </div>
                <div id="event-selection" class="section level4">
                <h4><span class="header-section-number">3.1.1.5</span> Event Selection</h4>
                <p>Given the mixture model structure expected for <span class="math inline">\(p ( \boldsymbol{x}|\boldsymbol{\theta} )\)</span> and the fact we are only interested in a small amount of the readout generating processes for each collision, because in general <span class="math inline">\(\phi_S \ll \phi_B \ll \phi_{\textrm{not-hard}}\)</span>, the effect of trigger or any other <em>event selection</em> should be considered. The role of event selection is to reduce the fraction of events that do not contain useful information for the inference task of interest. Trigger selection can be thought of as a technical requirement, reducing the total rate of detector readouts recorded to match the available hardware for data acquisition, as discussed in Section <a href="2-2-the-compact-muon-solenoid.html#sec:trigger">2.2.7</a>. The purpose of analysis selection, as will be discussed in Chapter <a href="5-search-for-anomalous-higgs-pair-production-with-cms.html#sec:higgs_pair">5</a>, is instead to reduce the expected contribution of background processes that are not well-modelled by simulation, as well as to the increase the expected fraction of signal events in synthetic counting likelihoods, such as those which will be detailed in Section <a href="3-1-statistical-modelling.html#sec:synthetic_likelihood">3.1.3.4</a>.</p>
                <p>In general mathematical terms, any deterministic event selection can be thought of as an indicator function <span class="math inline">\(\mathbb{1}_\mathcal{C} : \mathcal{X} \longrightarrow \{0,1\}\)</span>, of a given subset of the set of possible detector readouts <span class="math inline">\(\mathcal{C} \subseteq \mathcal{X}\)</span>. The indicator function <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span> can be defined as:</p>
                <p><span id="eq:indicator"><span class="math display">\[\mathbb{1}_\mathcal{C}(\boldsymbol{x}) =
                  \begin{cases}
                    1 \ \textrm{if} \ \mathbf{x} \in C \\
                    0 \ \textrm{if} \ \mathbf{x} \notin C \\
                \end{cases}
                \qquad(3.6)\]</span></span></p>
                <p>where the specific definition of such function depends on the definition of the subset <span class="math inline">\(\mathcal{C}\)</span>, e.g. a simple cut on a one-dimensional function <span class="math inline">\(f : \mathcal{X} \longrightarrow T \subseteq \mathcal{R}\)</span> of the readout <span class="math inline">\(f(\boldsymbol{x}) &gt; t_{{\textrm{cut}}}\)</span>. Any indicator function can also be viewed as a boolean predicate function, so the event selection can also be expressed as a combination of selection functions, i.e. if the set <span class="math inline">\(\mathcal{C}=\mathcal{A} \cap \mathcal{B}\)</span> is the intersection between two subsets, the indicator function of <span class="math inline">\(C\)</span> can be simply expressed as the product <span class="math inline">\(\mathbb{1}_\mathcal{C}=\mathbb{1}_\mathcal{A} \cdot \mathbb{1}_\mathcal{B}\)</span>. This framework is flexible enough to represent all deterministic event selections, and it could also be extended by an independent non-deterministic term without affecting the rest of the considerations presented in this chapter. A non-deterministic factor could be useful to model for example trigger prescales, which are trigger decisions based on randomly selecting a fraction of all the selected events to be recorded, ensuring that the total rate is manageable.</p>
                <p>In practice, a given selection <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span>, likely based on a composition of simple criteria, would have been imposed on the recorded detector readouts before any statistical analysis is carried out. The structure of the statistical model <span class="math inline">\(g(\boldsymbol{x} | \boldsymbol{\theta} )\)</span> resulting after applying an arbitrary selection <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span> on a mixture model as the one described in Equation <a href="3-1-statistical-modelling.html#eq:cond_density">3.1</a> can be obtained by multiplying the probability density by <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span>. After including the relevant normalisation term, the resulting probability distribution can be expressed as:</p>
                <p><span id="eq:mixture_after_cut"><span class="math display">\[ g(\boldsymbol{x} | \boldsymbol{\theta} ) = \frac{
                  \mathbb{1}_\mathcal{C}(\boldsymbol{x})
                  \sum^{K-1}_{j=0} \phi_j \ p_j ( \boldsymbol{x}|\boldsymbol{\theta})}{
                  \int \left (\mathbb{1}_\mathcal{C}(\boldsymbol{x})
                  \sum^{K-1}_{j=0} \phi_j \ p_j ( \boldsymbol{x}|\boldsymbol{\theta}) \right )
                  d \boldsymbol{x}}
                  =
                  \sum^{K-1}_{j=0} \left (  \frac{ \phi_j
                  \epsilon_j
                  }{
                  \sum^{K-1}_{j=0} \phi_j
                  \epsilon_j
                  } \right ) g_j (\boldsymbol{x}|\boldsymbol{\theta})
                \qquad(3.7)\]</span></span></p>
                <p>where <span class="math inline">\(g_j (\boldsymbol{x}|\boldsymbol{\theta}) = \mathbb{1}_\mathcal{C}(\boldsymbol{x}) p_j (\boldsymbol{x}|\boldsymbol{\theta})  / \epsilon_j\)</span> is the probability density function of each mixture component after the selection, <span class="math inline">\(\epsilon_j=\int \mathbb{1}_\mathcal{C}(\boldsymbol{x}) p_j (\boldsymbol{x}|\boldsymbol{\theta})\)</span> is the <em>efficiency</em> on the selection on each mixture, and the integral sign in the denominator in the last expression has been simplified by noting that <span class="math inline">\(\int g_j ( \boldsymbol{x}|\boldsymbol{\theta}) d \boldsymbol{x} = 1\)</span>. From Equation <a href="3-1-statistical-modelling.html#eq:mixture_after_cut">3.7</a> it becomes clear that the statistical model after any event selection is also a mixture model, whose mixture components are <span class="math inline">\(g_j (\boldsymbol{x}|\boldsymbol{\theta})\)</span> and mixture fractions are <span class="math inline">\(\chi_j=\phi_j\epsilon_j/\sum^{K-1}_{j=0} \phi_j\epsilon_j\)</span>. This fact will be very relevant to build statistical models of the observed data after an event event selection is in place.</p>
                <p>So far, no explicit assumptions on the probability distribution functions of each mixture component <span class="math inline">\(j\)</span> or the details of the event selection function <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span> have been considered, in order to keep the previously developed modelling framework as general as possible. In the next sections, it will become increasingly clear how <span class="math inline">\(p_j (\boldsymbol{x}|\boldsymbol{\theta})\)</span>, and in turn <span class="math inline">\(g_j (\boldsymbol{x}|\boldsymbol{\theta})\)</span> and the efficiency <span class="math inline">\(\epsilon_j\)</span>, can be modelled by generating simulated detector readouts produced by a given process <span class="math inline">\(j\)</span>.</p>
                </div>
                </div>
                <div id="simulation-as-generative-modelling" class="section level3">
                <h3><span class="header-section-number">3.1.2</span> Simulation as Generative Modelling</h3>
                <p>The physical principles underlying the simulation of detector readouts, or events, for a given hard proton-proton interaction process were reviewed in Section <a href="1-3-phenomenology-of-proton-collisions.html#sec:pheno">1.3</a> and Section <a href="2-3-event-simulation-and-reconstruction.html#sec:event">2.3</a>. Instead of focussing on the procedural details of event generation, the focus of this section is the study of the simulation chain as a generative statistical model, together with its basic structure and properties, that will be useful later to understand many analysis techniques that are commonly used in experimental particle physics.</p>
                <p>For simplicity, we will be considering the statistical model describing the distribution of observations of detector readouts before any event selection, what was referred to as <span class="math inline">\(p ( \boldsymbol{x}|\boldsymbol{\theta} )\)</span> in the previous section. Always taking into account that the distribution after any arbitrary deterministic event selection <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span> is also a mixture model (see Equation <a href="3-1-statistical-modelling.html#eq:mixture_after_cut">3.7</a>) and samples under the corresponding probability distribution functions <span class="math inline">\(g_j (\boldsymbol{x}|\boldsymbol{\theta})\)</span> and mixture fractions <span class="math inline">\(\chi_j\)</span> can easily obtained from the non-selected simulated events, as it is actually done in practice.</p>
                <div id="observable-and-latent-variables" class="section level4">
                <h4><span class="header-section-number">3.1.2.1</span> Observable and Latent Variables</h4>
                <p>The first step to build a generative statistical model is to define what are the observed variables and what are the hidden quantities, referred to as <em>latent variables</em>, that explain the structure of the data. For particle collider experiments, we may consider the full detector readout <span class="math inline">\(\boldsymbol{x} \in \mathcal{X} \subseteq \mathbb{R}^d\)</span> as the only observable variable, given that any other observable can be expressed as a function of the raw readout, as will be discussed in Section <a href="3-1-statistical-modelling.html#sec:dim_reduction">3.1.3</a>. The probability density function of the data <span class="math inline">\(p ( \boldsymbol{x}|\boldsymbol{\theta} )\)</span> from a generative standpoint can be written as an integration of the joint distribution <span class="math inline">\(p(\boldsymbol{x}, \boldsymbol{z} | \boldsymbol{\theta})\)</span> over all latent variables <span class="math inline">\(\boldsymbol{z}\)</span> of an event: <span id="eq:as_integration"><span class="math display">\[p ( \boldsymbol{x}|\boldsymbol{\theta} ) =
                \int p(\boldsymbol{x}, \boldsymbol{z} | \boldsymbol{\theta})d\boldsymbol{z}
                \qquad(3.8)\]</span></span> where <span class="math inline">\(\boldsymbol{\theta}\)</span> is a vector with all model parameters, which normally are global (same for all the observations) and include the theory parameters of interest as well as any other parameter that affect the detector readouts. While the true generative model of the data <span class="math inline">\(p(\boldsymbol{x}, \boldsymbol{z} | \boldsymbol{\theta})\)</span> is unknown, knowledge about the underlying physical processes described in in Section <a href="1-3-phenomenology-of-proton-collisions.html#sec:pheno">1.3</a> and Section <a href="2-3-event-simulation-and-reconstruction.html#sec:event">2.3</a> can be used to build a generative approximation of <span class="math inline">\(p(\boldsymbol{x}, \boldsymbol{z} | \boldsymbol{\theta})\)</span> which can describe the observed data realistically and be used to carry out inference on the parameters of interest.</p>
                <p>In fact, one of the most relevant latent variables at particle colliders has been already introduced with the generative definition of a mixture model in Equation <a href="3-1-statistical-modelling.html#eq:mixture_gen">3.3</a>, the mixture assignment integer <span class="math inline">\(z_i \in \{0, \dots, K -1 \}\)</span>. This latent variable represents which type of fundamental interaction occurred in the event, and is useful to exemplify the main property of latent variables: that they are not observed but can only (at most) be inferred. Let us consider the problem of finding out the type of interaction <span class="math inline">\(j\)</span> that caused a single detector readout observation <span class="math inline">\(\boldsymbol{x}_i\)</span>. As long as <span class="math inline">\(\boldsymbol{x}_i\)</span> is in the support space of more than one of the mixture components <span class="math inline">\(p_j( \boldsymbol{x}|\boldsymbol{\theta})\)</span>, which is almost always the case, only probabilistic statements about the type of interaction originating <span class="math inline">\(\boldsymbol{x}_i\)</span> can be made, even if the <span class="math inline">\(p_j( \boldsymbol{x}|\boldsymbol{\theta})\)</span> are known. In practice, <span class="math inline">\(p_j( \boldsymbol{x}|\boldsymbol{\theta})\)</span> are not known analytically so probabilistic classification techniques can be used to estimate the conditional probabilities based on simulated samples, as discussed in Chapter <a href="4-machine-learning-in-high-energy-physics.html#sec:machine_learning">4</a>.</p>
                </div>
                <div id="structure-of-generative-model" class="section level4">
                <h4><span class="header-section-number">3.1.2.2</span> Structure of Generative Model</h4>
                <p>Other than the basic mixture model structure, our understanding of the underlying physical process occurring in proton-proton collisions can be used to recognise additional structure in the generative model by means of factorising the joint distribution <span class="math inline">\(p(\boldsymbol{x}, \boldsymbol{z} | \boldsymbol{\theta})\)</span> in conditional factors matching the various simulation steps and their dependencies: <span id="eq:factor_joint"><span class="math display">\[p(\boldsymbol{x}, \boldsymbol{z} | \boldsymbol{\theta}) =
                p ( \boldsymbol{x} | \boldsymbol{z}_\textrm{d})
                p ( \boldsymbol{z}_\textrm{d} | \boldsymbol{z}_\textrm{s})
                p ( \boldsymbol{z}_\textrm{s} | \boldsymbol{z}_\textrm{p})
                \sum^{K-1}_{j=0} p ( z_i  = j |\boldsymbol{\theta})
                p ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}, z_i  = j)
                \qquad(3.9)\]</span></span> where each factor can be defined as follows:</p>
                <ul><li><span class="math inline">\(p( z_i = j|\boldsymbol{\theta}) = \phi_j(\boldsymbol{\theta})\)</span> is the probability of a given type of process <span class="math inline">\(j\)</span> occurring, which is usually a function of theory parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>.</li>
                <li><span class="math inline">\(p ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}, z_i = j)\)</span> is the conditional probability density of a given set of parton-level four-momenta particles (characterised by the latent representation <span class="math inline">\(\boldsymbol{z}_\textrm{p} \in \mathcal{Z}_\textrm{p}\)</span>) of being the outcome of a group of fundamental proton interaction processes <span class="math inline">\(pp \longrightarrow X\)</span> indexed by the latent variable <span class="math inline">\(z_i \in \mathcal{Z}_i\)</span>, which might also be a function of theory parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>.</li>
                <li><span class="math inline">\(p( \boldsymbol{z}_\textrm{s} | \boldsymbol{z}_\textrm{p})\)</span> is the conditional density of a given parton-shower outcome. <span class="math inline">\(\boldsymbol{z}_\textrm{s} \in \mathcal{Z}_\textrm{d}\)</span> as a function of the parton-level outcome.</li>
                <li><span class="math inline">\(p ( \boldsymbol{z}_\textrm{d} | \boldsymbol{z}_\textrm{s})\)</span> is the conditional density of a set of detector interactions and readout noise <span class="math inline">\(\boldsymbol{z}_\textrm{d} \in \mathcal{Z}_\textrm{d}\)</span> as a function of the parton-shower output.</li>
                <li><span class="math inline">\(p ( \boldsymbol{x} | \boldsymbol{z}_\textrm{d})\)</span> is the conditional density of a given detector readout <span class="math inline">\(\boldsymbol{x} \in \mathcal{X}\)</span> as a function of the detector material interactions and detector readout noise.</li>
                </ul><p>The dimensionality of the latent space greatly increases with each simulation step, from a single integer for <span class="math inline">\(\mathcal{Z}_i\)</span>, to <span class="math inline">\(\mathcal{O}(10)\)</span> parton four-momenta variables within <span class="math inline">\(\mathcal{Z}_p\)</span>, to <span class="math inline">\(\mathcal{O}(100)\)</span> after the parton-shower <span class="math inline">\(\mathcal{Z}_s\)</span>, and finally to <span class="math inline">\(\mathcal{O}(10^8)\)</span> in the detector interaction latent space <span class="math inline">\(\mathcal{Z}_d\)</span> and also the observable readout space <span class="math inline">\(\mathcal{X}\)</span>. In the factorisation presented in Equation <a href="3-1-statistical-modelling.html#eq:factor_joint">3.9</a>, the dependence on the parameters has only has been made explicit for <span class="math inline">\(p( z_i|\boldsymbol{\theta})\)</span> and <span class="math inline">\(p ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}, z_i)\)</span>, that is because the theoretical parameters of interest <span class="math inline">\(\boldsymbol{\theta}\)</span> often only affect the rate of the different fundamental processes and their differential distributions, which correspond to the mentioned conditional probability distributions. In the actual simulation chain, all conditional factors typically depend on additional parameters which might be uncertain, and whose effect and modelling will be discussed in Section <a href="3-1-statistical-modelling.html#sec:known_unknowns">3.1.4</a>.</p>
                <p>As previously mentioned, computer programs can be used to realistically simulate detector observations. For simulated observations, not only the final readout is observed, but all latent variables can be obtained from the intermediate steps of the generative chain. These variables, in particular <span class="math inline">\(\boldsymbol{z}_\textrm{p}\)</span> and <span class="math inline">\(\boldsymbol{z}_\textrm{s}\)</span>, are commonly referred as <em>generator level observables</em>, and are extremely useful to construct techniques that approximate the latent variables from the detector readouts. In fact, the whole simulation chain can be viewed as a probabilistic program <span class="citation">[<a href="references.html#ref-Casado:2017cif" role="doc-biblioref">86</a>], [<a href="references.html#ref-Baydin:2018npr" role="doc-biblioref">87</a>]</span>, thus each of the factors in Equation <a href="3-1-statistical-modelling.html#eq:factor_joint">3.9</a> can be further broken down as a sequence of random samples, which can be used to speed up latent variable inference based on the execution traces, i.e. recorded sequences of random numbers generated for each observation.</p>
                <p>Some joint factorisations are particularly useful for data analysis and simulation, such as the one making explicit the dependence between the differential partonic cross sections and the parton configuration in the collision, because it allows to factor out the density of the latent variables <span class="math inline">\(\boldsymbol{z}_\textrm{PDF}\)</span> associated with the parton components (i.e. flavour and momenta of each interacting parton and factorisation scale <span class="math inline">\(\mu_F^2\)</span>, as depicted in Section <a href="1-3-phenomenology-of-proton-collisions.html#sec:factorisation">1.3.3</a>). Each mixture component <span class="math inline">\(j\)</span> in Equation <a href="3-1-statistical-modelling.html#eq:factor_joint">3.9</a>, which represents a group of fundamental interactions between protons <span class="math inline">\(pp \longrightarrow X\)</span>, can be expressed as the product of the probability of a given parton configuration <span class="math inline">\(p(\boldsymbol{z}_\textrm{PDF}|\boldsymbol{\theta}_\textrm{PDF})\)</span> and a mixture over all parton configurations that can that produce <span class="math inline">\(pp \longrightarrow X\)</span>, referred as <span class="math inline">\(L\)</span> in the following expression: <span id="eq:pdf_factorisation"><span class="math display">\[
                p(z_i|\boldsymbol{\theta})
                \ p ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}, z_i) =
                p(\boldsymbol{z}_\textrm{PDF}|\boldsymbol{\theta}_\textrm{PDF})
                \sum^{g \in L} p(z_f = g| \boldsymbol{\theta}, z_\textrm{PDF})
                p ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}, z_f = g)
                \qquad(3.10)\]</span></span> where <span class="math inline">\(p(z_f = g| \boldsymbol{\theta}, z_\textrm{PDF})\)</span> is the relative probability of given partonic process <span class="math inline">\(g\)</span> given a parton configuration <span class="math inline">\(\boldsymbol{z}_\textrm{PDF}\)</span> and <span class="math inline">\(p(\boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}, z_f = g)\)</span> is the probability distribution function of the parton-level particles produced as a result of the interaction for a given partonic process <span class="math inline">\(g\)</span>, which is proportional to the partonic differential cross section <span class="math inline">\(d\sigma(ij \rightarrow X)\)</span>. This factorisation is basically a probabilistic model version of Equation <a href="1-3-phenomenology-of-proton-collisions.html#eq:qcd_factorisation">1.32</a>, dealing with the QCD factorisation of the parton distribution functions and the hard process differential cross section.</p>
                <p>Another relevant phenomenon that can be made explicit in the joint distribution <span class="math inline">\(p(\boldsymbol{x}, \boldsymbol{z} | \boldsymbol{\theta})\)</span> is the effect of multiple hadron interactions in the collision, or pileup, as discussed in Section <a href="2-1-the-large-hadron-collider.html#sec:pile_up">2.1.3</a>. Given that each proton-proton interaction is independent from the others, the effect of pileup interactions can be considered by augmenting the factor representing the conditional probability density of the detector interaction and noise as a function of the hard interaction parton shower output <span class="math inline">\(p ( \boldsymbol{z}_\textrm{d} | \boldsymbol{z}_\textrm{s})\)</span> as follows: <span id="eq:pileup_fact"><span class="math display">\[
                p ( \boldsymbol{z}_\textrm{d} | \boldsymbol{z}_\textrm{s}) =
                p(\boldsymbol{z}_\textrm{d} | \boldsymbol{z}_\textrm{s},\boldsymbol{z}_\textrm{pileup})
                p(\boldsymbol{z}_\textrm{pileup} | \boldsymbol{\theta}_\textrm{pileup})
                \qquad(3.11)\]</span></span> where <span class="math inline">\(\boldsymbol{z}_\textrm{pileup}\)</span> is a latent variable representing the details about the pileup interactions that happened in a given collision (i.e. number of interactions and their corresponding particle outcome), and <span class="math inline">\(\boldsymbol{\theta}_\textrm{pileup}\)</span> are the bunch crossing and luminosity parameters that affect the pileup distribution.</p>
                <p>Further structure in the generative model can be often found, depending on the process being generated, the modelling assumptions, and the latent space representation chosen. As an example, it is often useful to factorise out the latent subspace that depends directly on the subset of parameters of interest from those that do not. The conditional observations in that latent subspace can sometimes be analytically expressed, or their dimensionality may be low enough to use non-parametric density estimation techniques effectively, which can greatly simplify the modelling of changes in the parameters of interest.</p>
                </div>
                <div id="sec:re-weighting" class="section level4">
                <h4><span class="header-section-number">3.1.2.3</span> Simulated Observations</h4>
                <p>The mentioned mixing structure of the probability distribution function <span class="math inline">\(p(\boldsymbol{x} | \boldsymbol{\theta})\)</span> greatly simplifies the simulation of realistic observations, because large datasets <span class="math inline">\(S_j = \{\boldsymbol{x}_0,...,\boldsymbol{x}_m\}\)</span> of simulated observations for each type of interaction <span class="math inline">\(j\)</span> can be simulated before any event selection. The expected value of any measurable function of the detector readout <span class="math inline">\(f(\boldsymbol{x})\)</span> for events coming from a given process <span class="math inline">\(j\)</span> can be expressed as: <span id="eq:montecarlo_obs"><span class="math display">\[
                \mathop{\mathbb{E}}_{x \sim p_j ( \boldsymbol{x}|\boldsymbol{\theta} )}
                \left [ f(\boldsymbol{x}) \right ] =  \int f(\boldsymbol{x})
                p_j ( \boldsymbol{x}|\boldsymbol{\theta} ) d\boldsymbol{x} \approx
                \frac{1}{m} \sum^{\boldsymbol{x}_s \in S_j } f(\boldsymbol{x}_s)
                \qquad(3.12)\]</span></span> where the last terms approximates the integral as the sum over all stochastic simulations for a given process. The previous Monte Carlo approximation can be used to estimate the selection efficiency <span class="math inline">\(\epsilon_j\)</span>, as defined in Equation <a href="3-1-statistical-modelling.html#eq:mixture_after_cut">3.7</a>, after any deterministic event selection <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span>: <span id="eq:montecarlo_eff"><span class="math display">\[
                \epsilon_j = \mathop{\mathbb{E}}_{x \sim p_j ( \boldsymbol{x}|\boldsymbol{\theta} )}
                \left [ \mathbb{1}_\mathcal{C}(\boldsymbol{x}) \right ] =  \int
                \mathbb{1}_\mathcal{C}(\boldsymbol{x})
                p_j ( \boldsymbol{x}|\boldsymbol{\theta} ) d\boldsymbol{x} \approx
                \frac{1}{m} \sum^{\boldsymbol{x}_s \in S_j } \mathbb{1}_\mathcal{C}(\boldsymbol{x})
                \qquad(3.13)\]</span></span> which simply corresponds to the number of simulated observations that pass the selection divided by the total number of simulated observations <span class="math inline">\(m\)</span>. Lastly, the expected value of any measurable function <span class="math inline">\(f(\boldsymbol{x})\)</span> after a given event selection <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span> for events generated by a given process <span class="math inline">\(j\)</span> can be approximated by: <span id="eq:montecarlo_obs_sel"><span class="math display">\[
                \mathop{\mathbb{E}}_{x \sim g_j ( \boldsymbol{x}|\boldsymbol{\theta} )}
                \left [ f(\boldsymbol{x}) \right ] = \frac{1}{\epsilon_j }  \int f(\boldsymbol{x})
                \mathbb{1}_\mathcal{C}(\boldsymbol{x})
                p_j ( \boldsymbol{x}|\boldsymbol{\theta} ) d\boldsymbol{x} \approx
                \frac{1}{\epsilon_j m} \sum^{\boldsymbol{x}_s \in S_j } f(\boldsymbol{x}_s)
                \mathbb{1}_\mathcal{C}(\boldsymbol{x})
                \qquad(3.14)\]</span></span> which corresponds to the mean of <span class="math inline">\(f(\boldsymbol{x})\)</span> for all the events that passed the selection, noting that if all the events passed the selection (i.e. <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x}) = 1\)</span>), then Equation <a href="3-1-statistical-modelling.html#eq:montecarlo_obs">3.12</a> would be recovered.</p>
                <p>While we have been dealing independently with the estimation of arbitrary expected values for a given mixture component <span class="math inline">\(j\)</span>, the computation of expected values of any measurable function <span class="math inline">\(f(\boldsymbol{x})\)</span> under the total mixture distribution can be easily be expressed as function of expectations of mixture components: <span id="eq:montecarlo_obs_mix_sel"><span class="math display">\[
                \mathop{\mathbb{E}}_{x \sim g ( \boldsymbol{x}|\boldsymbol{\theta} )}
                \left [ f(\boldsymbol{x}) \right ] = \int f(\boldsymbol{x})
                \sum^{K-1}_{j=0} \chi_j g_j (\boldsymbol{x}|\boldsymbol{\theta})
                 d\boldsymbol{x} \approx
                \sum^{K-1}_{j=0} \chi_j
                \mathop{\mathbb{E}}_{x \sim g_j ( \boldsymbol{x}|\boldsymbol{\theta} )}
                \left [ f(\boldsymbol{x}) \right ]
                \qquad(3.15)\]</span></span> where <span class="math inline">\(\chi_j=\phi_j\epsilon_j/\sum^{K-1}_{j=0} \phi_j\epsilon_j\)</span> is the mixture fraction after selection (see Equation <a href="3-1-statistical-modelling.html#eq:mixture_after_cut">3.7</a>). While the problem of estimation of expected values might seem unrelated to the inference problem at hand, in Chapter <a href="3-1-statistical-modelling.html#sec:dim_reduction">3.1.3</a> it will become evident that the construction of non-parametric likelihoods of summary statistics can be reduced to the estimation of expected values.</p>
                <p>Oftentimes, the simulated observations are generated using a somewhat different probability distribution than that of experimental data, maybe because some of the generating parameters are not known precisely beforehand (e.g. the properties of pileup interactions). Alternatively, we might want to use a single set of simulated observations to realistically model observables corresponding to a different value of the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> or even to compute observables under a different process <span class="math inline">\(j\)</span>. Let us suppose that the samples were generated under <span class="math inline">\(p_Q(\boldsymbol{x} | \boldsymbol{\theta}_Q)\)</span> while we want to model samples under <span class="math inline">\(p_R(\boldsymbol{x} | \boldsymbol{\theta}_R)\)</span>. In that case, if both distributions have the same support, we can express the expectation value under the desired distribution as: <span id="eq:reweight_intractable"><span class="math display">\[
                \mathop{\mathbb{E}}_{x \sim p_R ( \boldsymbol{x}|\boldsymbol{\theta}_Q )}
                \left [ f(\boldsymbol{x}) \right ] =
                \frac {\int f(\boldsymbol{x})
                \frac{p_R ( \boldsymbol{x}|\boldsymbol{\theta}_R )}{
                p_Q ( \boldsymbol{x}|\boldsymbol{\theta}_Q )}
                p_Q ( \boldsymbol{x}|\boldsymbol{\theta}_Q )  d \boldsymbol{x}}{
                \int
                \frac{p_R ( \boldsymbol{x}|\boldsymbol{\theta}_R )}{
                p_Q ( \boldsymbol{x}|\boldsymbol{\theta}_Q )}
                p_Q ( \boldsymbol{x}|\boldsymbol{\theta}_Q )  d \boldsymbol{x}}
                \approx   \frac{\sum^{\boldsymbol{x}_s \in S_j }
                w(\boldsymbol{x}_s) f(\boldsymbol{x}_s)}{
                \sum^{\boldsymbol{x}_s \in S_j }
                w(\boldsymbol{x}_s)
                }
                \qquad(3.16)\]</span></span> which is analogous to what was done in Equation <a href="3-1-statistical-modelling.html#eq:montecarlo_obs">3.12</a>, but accounting for a weight <span class="math inline">\(w(\boldsymbol{x}_s) = p_R ( \boldsymbol{x}_s|\boldsymbol{\theta}_R )/ p_Q ( \boldsymbol{x}_s|\boldsymbol{\theta}_Q )\)</span> for each simulated observation. This technique can be also used together with an arbitrary event selection <span class="math inline">\(\mathbb{1}_\mathcal{C}(\boldsymbol{x})\)</span> simply by considering as event weight the product <span class="math inline">\(w_{\mathcal{C}}(\boldsymbol{x}_s) = \mathbb{1}_\mathcal{C} (\boldsymbol{x}) w(\boldsymbol{x}_s)\)</span>, which amounts to summing over the selected events. In particle physics experiments, the probability distribution functions <span class="math inline">\(p_Q(\boldsymbol{x} | \boldsymbol{\theta}_R)\)</span> and <span class="math inline">\(p_Q(\boldsymbol{x} | \boldsymbol{\theta}_R)\)</span> are most likely intractable, thus estimation of <span class="math inline">\(w_{\mathcal{C}}(\boldsymbol{x}_s)\)</span> has either to be carried out by non-parametric density estimation in a lower dimensional-space of the detector readouts (discussed in Section <a href="3-1-statistical-modelling.html#sec:dim_reduction">3.1.3</a>) or by directly estimating the density ratio via probabilistic classification as will be discussed in Chapter <a href="4-machine-learning-in-high-energy-physics.html#sec:machine_learning">4</a>.</p>
                <p>As previously mentioned, an advantage of using simulated observations is that the latent variables <span class="math inline">\(\mathcal{H}_j= \{\boldsymbol{z}_0,...,\boldsymbol{z}_m\}\)</span> for a given simulated set of observations <span class="math inline">\(S_j = \{\boldsymbol{x}_0,...,\boldsymbol{x}_{m-1}\}\)</span> are known. This allows to rewrite the weight <span class="math inline">\(w(\boldsymbol{x}_s,\boldsymbol{z}_s)\)</span> for a given event as the ratio of joint distributions: <span id="eq:latent_reweighting"><span class="math display">\[
                w(\boldsymbol{x}_s, \boldsymbol{z}_s) =
                \frac{p_R(\boldsymbol{x}_s,\boldsymbol{z}_s | \boldsymbol{\theta}_R)}{
                p_Q(\boldsymbol{x}_s,\boldsymbol{z}_s | \boldsymbol{\theta}_Q)
                } = \frac { p_R ( \boldsymbol{x} | \boldsymbol{z}_\textrm{d})
                p_R ( \boldsymbol{z}_\textrm{d} | \boldsymbol{z}_\textrm{s})
                p_R ( \boldsymbol{z}_\textrm{s} | \boldsymbol{z}_\textrm{p})
                p_R ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}_R) }{
                p_Q ( \boldsymbol{x} | \boldsymbol{z}_\textrm{d})
                p_Q ( \boldsymbol{z}_\textrm{d} | \boldsymbol{z}_\textrm{s})
                p_Q ( \boldsymbol{z}_\textrm{s} | \boldsymbol{z}_\textrm{p})
                p_Q ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}_Q)
                }
                \qquad(3.17)\]</span></span> where the last term is an expansion of each joint distribution as a product of the conditional distributions discussed in Equation <a href="3-1-statistical-modelling.html#eq:factor_joint">3.9</a>. If the difference between <span class="math inline">\(p_R(\boldsymbol{x} | \boldsymbol{\theta}_R)\)</span> and <span class="math inline">\(p_Q(\boldsymbol{x} | \boldsymbol{\theta}_Q)\)</span> is contained in one of the factors of the joint distribution, which is often the case, most of the factors in Equation <a href="3-1-statistical-modelling.html#eq:latent_reweighting">3.17</a> cancel out and we are left with a much simpler problem of density ratio estimation in the latent space. This is often what is done to model the effect of a different pileup distribution or alternative parton distribution functions, further factoring the joint distribution to include explicit dependencies with respect to <span class="math inline">\(\boldsymbol{z}_\textrm{pileup}\)</span> or <span class="math inline">\(\boldsymbol{z}_\textrm{PDF}\)</span>, as done in Equation <a href="3-1-statistical-modelling.html#eq:pileup_fact">3.11</a> and Equation <a href="3-1-statistical-modelling.html#eq:pdf_factorisation">3.10</a> respectively. The case when the difference between distributions is contained in a subset of the parton-level latent variables is one of special relevance, because the event weight for a given event <span class="math inline">\(w(\boldsymbol{z}_s)\)</span> can be expressed as the ratio: <span id="eq:gen_level_reweighting"><span class="math display">\[
                w(\boldsymbol{z}_s) = \frac{p_R ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}_R)}{p_Q ( \boldsymbol{z}_\textrm{p}|\boldsymbol{\theta}_Q)}
                \qquad(3.18)\]</span></span> which is referred to as <em>generator-level re-weighting</em>, a procedure that in some cases can even be done analytically. The concept of <em>re-weighting</em> will be useful to model different parameter points in Chapter <a href="5-search-for-anomalous-higgs-pair-production-with-cms.html#sec:higgs_pair">5</a> with a single set of simulated observations as well as to understand how the effect of varying parameters can be modelled via differentiable transformations in Chapter <a href="6-inference-aware-neural-optimisation.html#sec:inferno">6</a>.</p>
                </div>
                </div>
                <div id="sec:dim_reduction" class="section level3">
                <h3><span class="header-section-number">3.1.3</span> Dimensionality Reduction</h3>
                <p>In the previous overview of the basic statistical modelling principles of experimental high-energy physics, the structure and properties of the probability distribution of the full detector readout <span class="math inline">\(\boldsymbol{x} \in \mathcal{X}\)</span> have been considered. The consideration of the detector readout as single observable variable <span class="math inline">\(\boldsymbol{x}\)</span> in the generative model greatly simplifies the modelling narrative, plus also allows to include the effect of any arbitrary event selection as a deterministic function <span class="math inline">\(\mathbb{1}_\mathcal{C} (\boldsymbol{x})\)</span>. Nevertheless, the high-dimensionality of the readout space <span class="math inline">\(\boldsymbol{x} \in \mathcal{X}\)</span> (i.e. <span class="math inline">\(\mathcal{O}(10^8)\)</span>) complicates its direct use when comparing simulated and recorded observations, which is crucial when carrying out any type of statistical inference.</p>
                <p>The high-dimensionality of the raw detector readout space <span class="math inline">\(\boldsymbol{x} \in \mathcal{X}\)</span> also makes it very difficult to specify an effective event selection <span class="math inline">\(\mathbb{1}_\mathcal{C} (\boldsymbol{x})\)</span> that is able to reduce the contributions from non-interesting or not well-modelled background processes. This motivates the use of a dimensionality reduction function <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x}) : \mathcal{X} \longrightarrow \mathcal{Y}\)</span>, from the raw detector readout space <span class="math inline">\(\mathcal{X} \subseteq \mathbb{R}^{d}\)</span> to a lower dimensional space <span class="math inline">\(\mathcal{Y} \subseteq \mathbb{R}^{b}\)</span>. Here <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x})\)</span> represents any deterministic function of the detector readout, but in practice it can be implemented by a series of consecutive transformations.</p>
                <p>Let us denote as <span class="math inline">\(\boldsymbol{y} \in \mathcal{Y}\)</span> the resulting variable after the transformation <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x})\)</span> is applied to the observed detector readout. If the function <span class="math inline">\(\boldsymbol{f}\)</span> is differentiable and bijective (i.e. there is a one-to-one correspondence between <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(\boldsymbol{y}\)</span>), the probability density distribution function of <span class="math inline">\(\boldsymbol{y}\)</span> could be obtained as: <span id="eq:change_of_vars"><span class="math display">\[
                p(\boldsymbol{y} | \boldsymbol{\theta}) =
                p(\boldsymbol{x} | \boldsymbol{\theta})
                \left | \det \frac{d\boldsymbol{x} }{d\boldsymbol{y} } \right |
                \qquad(3.19)\]</span></span> where the last term is the Jacobian determinant of the inverse of <span class="math inline">\(\boldsymbol{f}\)</span>. The transformations commonly used in particle colliders are non-bijective and sometimes non-differentiable, plus Equation <a href="3-1-statistical-modelling.html#eq:change_of_vars">3.19</a> is in any case of little use when <span class="math inline">\(p(\boldsymbol{x} | \boldsymbol{\theta})\)</span> is intractable. However, the expectation value of <span class="math inline">\(\boldsymbol{y}\)</span> as well any other deterministic transformation of the detector readout <span class="math inline">\(\boldsymbol{x}\)</span> after any arbitrary event selection <span class="math inline">\(\mathbb{1}_\mathcal{C} (\boldsymbol{x})\)</span> can be obtained using simulated samples for a given interaction process as shown in Equation <a href="3-1-statistical-modelling.html#eq:montecarlo_obs_sel">3.14}</a>, independently of whether the transformation is invertible or differentiable. In the rest of this section, the main procedures followed to reduce the dimensionality of the observable space and its objectives from a statistical perspective will be discussed.</p>
                <div id="sec:event_reco_stat" class="section level4">
                <h4><span class="header-section-number">3.1.3.1</span> Event Reconstruction</h4>
                <p>The methods of event reconstruction, as described in Section <a href="2-3-event-simulation-and-reconstruction.html#sec:event_reco">2.3.3</a>, provide a very efficient way to transform the high-dimensional detector readout to a lower-dimensional space that can more easily be interpreted from a physical standpoint. In fact, reconstruction can be viewed as a complex procedural technique of inference on a subset of the latent variables given the detector readout <span class="math inline">\(\boldsymbol{x}\)</span> of an event. These methods attempt to walk back the generative chain described in Equation <a href="3-1-statistical-modelling.html#eq:factor_joint">3.9</a> to recover the subset of the parton-level <span class="math inline">\(\boldsymbol{z}_\textrm{p}\)</span> (and <span class="math inline">\(\boldsymbol{z}_\textrm{s}\)</span> or <span class="math inline">\(\boldsymbol{z}_\textrm{d}\)</span> in some cases) that strongly depends on the detector readouts, providing a compressed summary of the information in the event about the parameters of interest <span class="math inline">\(\boldsymbol{\theta}\)</span>. The dimensionality of the output of the reconstruction procedure <span class="math inline">\(\boldsymbol{y}_\textrm{reco}\)</span> depends on the subset of variables considered for each physical object, which typically amounts to a total of <span class="math inline">\(\mathcal{O}(100)\)</span> dimensions, which is a significant reduction from <span class="math inline">\(\dim(\mathcal{X}) \approx \mathcal{O}(10^8)\)</span>.</p>
                <p>Due to the detector noise and characteristics, the reconstruction function <span class="math inline">\(\boldsymbol{f}_\textrm{reco}(\boldsymbol{x}) : \mathcal{X} \longrightarrow \mathcal{Y}_\textrm{reco}\)</span> cannot fully recover <span class="math inline">\(\boldsymbol{z}_\textrm{p} \in \mathcal{Z}_\textrm{p}\)</span>. This is the case for neutrinos that leave the detector undetected, when the measured four-momentum of a given particle differs from the real value, or when the reconstructed particle does not even exist in <span class="math inline">\(\boldsymbol{z}_\textrm{p}\)</span>. Simulated events can then be used to make calibrated probabilistic statements of the resulting reconstructed physical objects and their relation with the actual unobserved particles going through the detector. Particle identification (e.g. jet b-tagging) and fine-tuned momentum regressions on the reconstructed objects can also be thought of as inference of latent variables, which amounts to using the additional detector information around an object to measure more precisely its properties. These properties include the type of particle that produced the detector readouts clustered for particle identification, or a more precise determination of the momentum for particle regression.</p>
                <p>One aspect of the generative model that complicates both reconstruction and statistical inference which has not been discussed yet is that efficient representations of the latent space of simulated events are not easily represented as a fixed-size real vector <span class="math inline">\(\boldsymbol{z} \in \mathcal{Z} \subseteq \mathbb{R}^b\)</span>. Let us consider as an example the parton-level latent information <span class="math inline">\(\boldsymbol{z}_\textrm{p}\)</span>, which amounts to a short list of produced particles. The total number of particles and the number of particles of each type are variable, thus <span class="math inline">\(\boldsymbol{z}_\textrm{p}\)</span> is better represented by a set (or several sets, one for each particle type): <span id="eq:set_parton"><span class="math display">\[
                \boldsymbol{z}_\textrm{p}^\textrm{set} =
                 \{ \boldsymbol{z}_\textrm{p}^i \ | \
                  i \in \{{1,...,n_\textrm{p}} \} \}
                \qquad(3.20)\]</span></span> where <span class="math inline">\(n_\textrm{p}\)</span> is the total number of particles produced at parton-level and <span class="math inline">\(\boldsymbol{z}_\textrm{p}^\textrm{i}\)</span> are the latent variables associated to each particle (i.e. type, four-momenta, charge, colour and spin). A similar set structure can be attributed to latent variables describing long-lived particles after the parton-shower <span class="math inline">\(\boldsymbol{z}_\textrm{s}\)</span>, while additional variables might be associated to each particle (e.g. production vertex) and total number and type diversity would be considerably larger. Because the number of particles and their type greatly varies between different interaction processes, the mapping this structure to observable variable space is very useful. In fact, the result of general event reconstruction process at CMS can be expressed also as a set of physical objects: <span id="eq:set_reco"><span class="math display">\[
                \boldsymbol{y}_\textrm{reco}^\textrm{set} =
                 \{ \boldsymbol{y}_\textrm{reco}^\textrm{i} \ | \
                  i \in \{{1,...,n_\textrm{reco}} \} \}
                \qquad(3.21)\]</span></span> where <span class="math inline">\(n_\textrm{reco}\)</span> is the total number of particles, <span class="math inline">\(\boldsymbol{y}_\textrm{reco}^\textrm{i}\)</span> are the reconstructed variables for each physical object (i.e. reconstructed type, reconstructed four-momenta, reconstructed charge and any other reconstructed attributes). The calibration between the reconstructed physical objects <span class="math inline">\(\boldsymbol{y}_\textrm{reco}^\textrm{set}\)</span> and the actual particles produced in the collision <span class="math inline">\(\boldsymbol{z}_\textrm{p/s}^\textrm{set}\)</span> hence amounts to matching set elements (typically based on a <span class="math inline">\(\Delta R\)</span> distance criterion, see Section <a href="2-2-the-compact-muon-solenoid.html#sec:exp_geom">2.2.1</a>) and the comparison of their reconstructed and generated attributes.</p>
                <p>The fact that both reconstructed and latent spaces have a variable-size set structure greatly complicates the application of inference and learning techniques directly based on <span class="math inline">\(\boldsymbol{y}_\textrm{reco}^\textrm{set}\)</span>, because they often can only deal with a fixed-size vector of real numbers <span class="math inline">\(\mathbb{R}^b\)</span>. Similarly to what is done for event selection, often the elements in the set of reconstructed objects in an event are reduced by imposing a given condition based on their attributes (e.g. type, isolation or momenta). There exist naive ways to embed a set such as <span class="math inline">\(\boldsymbol{y}_\textrm{reco}^\textrm{set}\)</span> as a fixed-size vector <span class="math inline">\(\mathbb{R}^b\)</span>, such as taking the relevant attributes of the first <span class="math inline">\(n_\textrm{sel}\)</span> objects according to a specific ordering convention after a given <em>object selection</em> and possibly padding with zeros or alternative numerical values the elements that do not exist for a given event. Some of the newer machine learning techniques that will be presented in Chapter <a href="4-machine-learning-in-high-energy-physics.html#sec:machine_learning">4</a> can deal with variable-size input, such as sequences, sets or graphs inputs, by <em>embedding</em> them in vector representations internally, providing new ways to deal with the mentioned representational issue.</p>
                </div>
                <div id="sec:summary_statistic" class="section level4">
                <h4><span class="header-section-number">3.1.3.2</span> Summary Statistics</h4>
                <p>The attributes of the subset of reconstructed objects selected in an event for a given analysis, often as a fixed-size vector representation <span class="math inline">\(\boldsymbol{y}_\textrm{sel} \in \mathcal{Z}_\textrm{sel} \subseteq \mathbb{R}^{b}\)</span>, are often still too high-dimensional to be considered directly for statistical inference. The effectiveness of the likelihood-free techniques that will be presented later in this chapter strongly depend on the dimensionality of the observable space considered. Hence, it is desirable to further combine the reconstructed outputs in a lower dimensional <em>summary statistic</em>, which can be either a function of each single observation or a set of multiple observations, so simpler statistical models that relate the parameters of interest with the observations can be constructed.</p>
                <p>Until now, we have been dealing with the problem of how a single event is distributed <span class="math inline">\(p ( \boldsymbol{x}|\boldsymbol{\theta})\)</span>, however in practice a collection <span class="math inline">\(D = \{\boldsymbol{x}_0,...,\boldsymbol{x}_n\}\)</span> of events is considered for inference. Let us first consider again the set <span class="math inline">\(D\)</span>, before any trigger or event selection, similarly to what was done at the beginning of Section <a href="3-1-statistical-modelling.html#sec:model_overview">3.1.1</a>. Because of the independence between events, the probability density of a given set <span class="math inline">\(D\)</span> can be expressed as the product of individual probability densities for each event <span class="math inline">\(\boldsymbol{x}_i\)</span>: <span id="eq:before_sel_prod"><span class="math display">\[
                p(D | \boldsymbol{\theta}) = \prod^{\boldsymbol{x}_i \in D}
                p ( \boldsymbol{x}_i|\boldsymbol{\theta} )
                \qquad(3.22)\]</span></span> where <span class="math inline">\(p ( \boldsymbol{x}_i|\boldsymbol{\theta} )\)</span> can only be modelled realistically by forward simulation, and has the mixture model structure and latent factorisation discussed before. After an arbitrary event selection <span class="math inline">\(\mathbb{1}_\mathcal{C} (\boldsymbol{x})\)</span>, only a subset of events <span class="math inline">\(D_\mathcal{C} = \{\boldsymbol{x}_0,...,\boldsymbol{x}_{n_\mathcal{C}}\} \subseteq D\)</span> remain. These events are also independent, so their probability density can be expressed as: <span id="eq:after_sel_prod"><span class="math display">\[
                g(D_\mathcal{C} | \boldsymbol{\theta}) = \prod^{\boldsymbol{x}_i \in D_\mathcal{C}}
                g ( \boldsymbol{x}_i|\boldsymbol{\theta} )
                \qquad(3.23)\]</span></span> where the dependence between the distribution function after the event selection <span class="math inline">\(g ( \boldsymbol{x}_i|\boldsymbol{\theta} )\)</span> and that before <span class="math inline">\(p ( \boldsymbol{x}_i|\boldsymbol{\theta} )\)</span> was already described in Equation <a href="3-1-statistical-modelling.html#eq:mixture_after_cut">3.7</a>. If we are only focussed on the probability distribution of the events in <span class="math inline">\(D_\mathcal{C}\)</span>, we would be neglecting an important quantity that can also provide information about the parameters of interest: the total number of events that pass the event selection <span class="math inline">\(n_\mathcal{C}\)</span>. Because this quantity depends on the set of recorded readouts <span class="math inline">\(\mathcal{D}\)</span>, where each individual readout <span class="math inline">\(\boldsymbol{x}_i\)</span> is assumed to be an independent and identically distributed variable, the total number of selected events <span class="math inline">\(n_C\)</span> after a deterministic selection <span class="math inline">\(\mathbb{1}_\mathcal{C} (\boldsymbol{x})\)</span> can be modelled using a binomial distribution: <span id="eq:binomial_selection"><span class="math display">\[
                p( n_\mathcal{C} | n, \boldsymbol{\theta}) = \textrm{Binomial}(n, \epsilon)
                \approx \textrm{Poisson}(n\epsilon)
                \qquad(3.24)\]</span></span> where <span class="math inline">\(n\)</span> is the total number of events, and the dependence on the parameters is contained in the total efficiency <span class="math inline">\(\epsilon\)</span>, i.e. probability <span class="math inline">\(\boldsymbol{x} \sim p(\boldsymbol{x}|\boldsymbol{\theta})\)</span> of passing the selection criteria, that can be defined as <span class="math inline">\(\epsilon = \int \mathbb{1}_\mathcal{C}(\boldsymbol{x}) p (\boldsymbol{x}|\boldsymbol{\theta})\)</span>. The Poisson approximation is justified because the number of trials <span class="math inline">\(n\)</span> is sufficiently large (i.e. 40 million bunch crossings per second) and the total selection efficiencies <span class="math inline">\(\epsilon \leq 0.000025\)</span> already at trigger level, as discussed in Section <a href="2-2-the-compact-muon-solenoid.html#sec:trigger">2.2.7</a>. This type of stochastic process is also referred to in the literature as multi-dimensional homogenous Poisson point process <span class="citation">[<a href="references.html#ref-Gardiner:732221" role="doc-biblioref">88</a>]</span>. The expected value of <span class="math inline">\(n_C\)</span> coincides with the Poisson mean <span class="math inline">\(n\epsilon\)</span>. It could be more intuitively linked with the parameters of interest <span class="math inline">\(\theta\)</span> by making explicit the contributions from the different mixture processes: <span id="eq:exp_selected"><span class="math display">\[
                \mathop{\mathbb{E}}_{D \sim p ( D |\boldsymbol{\theta} )}
                \left [ n_\mathcal{C} \right ] = n \sum^{K-1}_{j=0} \phi_j
                \mathop{\mathbb{E}}_{x \sim p_j ( \boldsymbol{x}|\boldsymbol{\theta} )}
                \left [\mathbb{1}_\mathcal{C} (\boldsymbol{x}) \right ] =
                n \sum^{K-1}_{j=0} \phi_j \epsilon_j
                \qquad(3.25)\]</span></span> where the efficiency for each process <span class="math inline">\(\epsilon_j= \int \mathbb{1}_\mathcal{C}(\boldsymbol{x}) p_j (\boldsymbol{x}|\boldsymbol{\theta})\)</span> can be estimated using simulated observations as shown in Equation <a href="3-1-statistical-modelling.html#eq:montecarlo_eff">3.13</a>. In principle, all possible processes <span class="math inline">\(j\)</span> that could occur have to be considered, i.e. cases when no hard collision occurred as well as the inclusive contribution of each possible hard process, as described in Equation <a href="3-1-statistical-modelling.html#eq:hard_prob">3.4</a>. However, if the product of the expected probability of a given process occurring <span class="math inline">\(\phi_j\)</span> and the event selection efficiency <span class="math inline">\(\epsilon_j\)</span> is low enough relative to the total efficiency <span class="math inline">\(\epsilon=\sum^{K-1}_{j=0} \phi_j \epsilon_j\)</span>, the effect of those mixture components can be safely neglected.</p>
                <p>The situation discussed above is often the case for events where no hard collision occurred after some basic event selection, that is <span class="math inline">\(\epsilon_\textrm{not-hard} \approx 0\)</span> so it can thus be neglected. For the subset of bunch crossings where hard interactions occur, the probability of a given type of interaction before any event selection might be expressed as the product of its cross section <span class="math inline">\(\sigma_j\)</span> by the total integrated luminosity during the data taking period <span class="math inline">\(\mathcal{L}_\textrm{int}\)</span> divided by the total number of bunch crossings, thus the expected value for number of observations <span class="math inline">\(n_\mathcal{C}\)</span> after an event selection that reduces to a negligible fraction the contribution of non-hard processes <span class="math inline">\(\mathbb{1}_\mathcal{C} (\boldsymbol{x})\)</span> can also be expressed as: <span id="eq:exp_cross_section"><span class="math display">\[
                \mathop{\mathbb{E}}_{D \sim p ( D |\boldsymbol{\theta} )}
                \left [ n_\mathcal{C} \right ] =
                n \sum^{K-1}_{j=0} \frac{\mathcal{L}_\textrm{int} \sigma_j}{n }\epsilon_j =
                \mathcal{L}_\textrm{int} \sum^{K-1}_{j=0}  \ \sigma_j \  \epsilon_j
                \qquad(3.26)\]</span></span> where <span class="math inline">\(n_j=\mathcal{L}_\textrm{int} \ \sigma_j \  \epsilon_j\)</span> is the expected number of events coming from a given process <span class="math inline">\(j\)</span>, that can be estimated with theoretical input regarding <span class="math inline">\(\sigma_j\)</span>, simulated observations to estimate <span class="math inline">\(\epsilon_j\)</span> and an experimental measurement of the luminosity <span class="math inline">\(\mathcal{L}\)</span>.</p>
                <p>The number of observations <span class="math inline">\(n_\mathcal{C}\)</span> that pass a given event selection <span class="math inline">\(\mathbb{1}_\mathcal{C} (\boldsymbol{x})\)</span>, which normally includes trigger and some additional analysis dependent selection, is the quantity that serves as the basis of the simplest statistical model used in particle physics to link theoretical parameters and observations. This type of summary statistic is very effective when the parameter of interest is the cross section of a single process <span class="math inline">\(\sigma_S\)</span> and the rest of background processes are well modelled by theoretical predictions and simulated observations. In that case, if all parameters but <span class="math inline">\(\sigma_S\)</span> are known, a <em>cut-and-count</em> sample-based likelihood can be built based on Equation <a href="3-1-statistical-modelling.html#eq:binomial_selection">3.24</a>, corresponding to the following probability density function: <span id="eq:poisson_simple"><span class="math display">\[
                p ( n_\mathcal{C} | \sigma_S) = \textrm{Poisson}
                \left (\sigma_s\epsilon_s + \sum^{j \in B} \sigma_j\epsilon_j \right)
                \qquad(3.27)\]</span></span> which can be used to carry out statistical inference about <span class="math inline">\(\sigma_S\)</span> given an observed number of events that pass the event selection <span class="math inline">\(n_\mathcal{C}^\textrm{obs}\)</span>, using classical techniques.</p>
                <p>The previous concept can be applied to several disjoint subsets of <span class="math inline">\(\mathcal{X}\)</span> simultaneously <span class="math inline">\(T=\{\mathcal{C}_0,...,\mathcal{C}_b\}\)</span>, each characterised by a different indicator function <span class="math inline">\(\mathbb{1}_{\mathcal{C}_t} (\boldsymbol{x})\)</span> defining an arbitrary event selection, as long as their intersection is null. The probability function for the variable <span class="math inline">\(\boldsymbol{n}_T = \{n_{\mathcal{C}_0},...,n_{\mathcal{C}_b}\}\)</span>, given that each <span class="math inline">\(n_{\mathcal{C}_i}\)</span> is independent, can be obtained as: <span id="eq:poisson_multichannel"><span class="math display">\[
                p ( \boldsymbol{n}_T | \boldsymbol{\theta}) = \prod^{\mathcal{C}_i \in T}
                \textrm{Poisson}
                \left (\sum^{j \in H} n^{\mathcal{C}_i}_j(\boldsymbol{\theta}) \right )
                \qquad(3.28)\]</span></span> where <span class="math inline">\(n^{\mathcal{C}_i}_j(\boldsymbol{\theta})\)</span> is the expected number of observed events coming from process <span class="math inline">\(j\)</span> after the selection <span class="math inline">\(\mathcal{C}_i\)</span>. As long as a parametrisation of <span class="math inline">\(n^{\mathcal{C}_i}_j(\boldsymbol{\theta})\)</span> exists, which can be often estimated as <span class="math inline">\(n^{\mathcal{C}_i}_j(\boldsymbol{\theta}) =\mathcal{L} \ \sigma_j \
                \epsilon^{\mathcal{C}_i}_j(\boldsymbol{\theta})\)</span>, Equation <a href="3-1-statistical-modelling.html#eq:poisson_multichannel">3.28</a> can be used to construct a likelihood to carry out inference on the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> based on the observed value of the sample summary statistic <span class="math inline">\(\boldsymbol{n}_T^\textrm{obs}\)</span>.</p>
                </div>
                <div id="sec:suff_stats" class="section level4">
                <h4><span class="header-section-number">3.1.3.3</span> Sufficient Statistics</h4>
                <p>The selection count vector <span class="math inline">\(\boldsymbol{n}_T^\textrm{obs}(D)\)</span>, which has not been specified yet, could be also written as a sum over a function <span class="math inline">\(\boldsymbol{n}_T(\boldsymbol{x}) : \mathcal{X} \subseteq \mathbb{R}^{d} \longrightarrow \mathcal{Y} \subseteq \{0,1\}^b \subset \mathbb{R}^{b}\)</span> applied for each event in <span class="math inline">\(D = \{\boldsymbol{x}_0,...,\boldsymbol{x}_n\}\)</span> as follows: <span id="eq:sum_count_vector"><span class="math display">\[
                \boldsymbol{n}_T^\textrm{obs}(D) = \sum^{\boldsymbol{x}_i \in D} \boldsymbol{n}_T(\boldsymbol{x})
                \qquad(3.29)\]</span></span> where <span class="math inline">\(\boldsymbol{n}_T^\textrm{obs}(D)\)</span> could be described as a summary statistic of the whole collection of observations while <span class="math inline">\(\boldsymbol{n}_T(\boldsymbol{x}_i)\)</span> summarises a single event <span class="math inline">\(\boldsymbol{x}_i\)</span>.</p>
                <p>There are infinite ways to choose a lower-dimensional summary statistic of the detector readout <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x}) : \mathcal{X} \subseteq \mathbb{R}^{d} \longrightarrow \mathcal{Y}\subseteq \mathbb{R}^{b}\)</span>. Functions of the type <span class="math inline">\(\boldsymbol{n}_T(\boldsymbol{x})\)</span> are only a reduced subset, yet still infinite, of the possible space of functions. Regardless of the likelihood-free inference methods considered (see Section <a href="3-2-statistical-inference.html#sec:stat_inf">3.2</a>), the need of a low-dimensional summary statistic can be thought as an effective consequence of the <em>curse of dimensionality</em>, because the number of simulated observations required to realistically model the probability density function or compute useful distance measures rapidly increases with the number of dimensions.</p>
                <p>In general, the selection of a summary statistic <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x})\)</span> is far from trivial, and naive choices can lead to large losses of useful information about the parameters of interest <span class="math inline">\(\boldsymbol{\theta}\)</span>. From classical statistics, we can define a <em>sufficient summary statistic</em> as the function of the set of observations that can be used for carrying out inference about the model parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> of a given statistical model in place of the original dataset without losing information <span class="citation">[<a href="references.html#ref-hogg1995introduction" role="doc-biblioref">89</a>]</span>. Such a sufficient statistic contains all the information in the observed sample useful to compute any estimate on the model parameters, and no complementary statistic can add any additional information about <span class="math inline">\(\boldsymbol{\theta}\)</span> contained in the set of observations. Sufficient statistics can be formally characterised using the Fisher-Neyman factorisation criterion, which states that a summary statistic <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x})\)</span> is sufficient for the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> if and only if the probability distribution function of <span class="math inline">\(\boldsymbol{x}\)</span> can be factorised as follows: <span id="eq:sufficient_single"><span class="math display">\[
                p(\boldsymbol{x} | \boldsymbol{\theta}) =
                q(\boldsymbol{x})
                r(\boldsymbol{s}(\boldsymbol{x}) | \boldsymbol{\theta})
                \qquad(3.30)\]</span></span> where <span class="math inline">\(q(\boldsymbol{x})\)</span> is a non-negative function that does not depend on the parameters and <span class="math inline">\(r(\boldsymbol{x})\)</span> is also a non-negative function for which the dependence on the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> is a function of the summary statistic <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x})\)</span>. The identity function <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x})=\boldsymbol{x}\)</span> is a sufficient summary statistic according to the theorem in Equation <a href="3-1-statistical-modelling.html#eq:sufficient_single">3.30</a>, however we are only interested in summaries that reduce the original data dimensionality without losing of useful information about the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
                <p>The definition of sufficiency can also be applied to a set of observations <span class="math inline">\(D = \{\boldsymbol{x}_0,...,\boldsymbol{x}_n\}\)</span>. In fact if we assume they are independent and identically distributed, and <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x})\)</span> is sufficient for each observation <span class="math inline">\(\boldsymbol{x}_i\)</span>, we may rewrite Equation <a href="3-1-statistical-modelling.html#eq:before_sel_prod">3.22</a> as: <span class="math display">\[
                p ( D |\boldsymbol{\theta}) =
                \prod^{\boldsymbol{x}_i \in D} q(\boldsymbol{x})
                \prod^{\boldsymbol{x}_i \in D}
                r(\boldsymbol{s}(\boldsymbol{x}_i) | \boldsymbol{\theta}) =
                q(D) r(\boldsymbol{s}(D)| \boldsymbol{\theta})
                \]</span> where the set of sufficient summary statistics for each observation is a sufficient summary statistic for the whole dataset <span class="math inline">\(\boldsymbol{s}(D) = \{ \ \boldsymbol{s}(\boldsymbol{x}_i) \ | \ \forall \boldsymbol{x}_i \in D \}\)</span> and the dependence on the summary statistic is contained as the product of independent factors for each observation.</p>
                <p>Because <span class="math inline">\(p(\boldsymbol{x} | \boldsymbol{\theta})\)</span> is not available in closed form in particle collider experiments, the general task of finding a sufficient summary statistic that reduces the dimensionality cannot be tackled directly by analytic means. However, for finite mixture models where the only model parameters are a function of the mixture coefficients <span class="math inline">\(\phi_j\)</span>, probabilistic classification can be used to obtain (approximate) sufficient summary statistics. We will return to this topic in Chapter <a href="4-machine-learning-in-high-energy-physics.html#sec:machine_learning">4</a>. When the parameters of interest or additional unknown parameters affect the mixture components <span class="math inline">\(p_j(\boldsymbol{x} | \boldsymbol{\theta})\)</span>, the construction of sufficient summary statistics cannot be addressed directly, thus a fraction information about the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> is very likely to be lost in the dimensionality reduction step. An automated way to obtain powerful summary statistics in those cases using machine learning techniques will be presented in Chapter <a href="6-inference-aware-neural-optimisation.html#sec:inferno">6</a>.</p>
                </div>
                <div id="sec:synthetic_likelihood" class="section level4">
                <h4><span class="header-section-number">3.1.3.4</span> Synthetic Likelihood</h4>
                <p>The advantage of using lower-dimensional summary statistics <span class="math inline">\(\boldsymbol{s}(D) : \mathcal{X}_D \subseteq \mathbb{R}^{d\times n} \longrightarrow \mathcal{Y}_D \subseteq \mathbb{R}^{b\times n}\)</span> of the detector readout collected by the experiment is that often the generative model of <span class="math inline">\(p(\boldsymbol{x} | \boldsymbol{\theta})\)</span> can be used to build non-parametric likelihoods of <span class="math inline">\(s(D)\)</span> that link the observations with the model parameters, so classical inference algorithms can be used. This likelihoods are referred here as synthetic because they are not based on the actual generative model of <span class="math inline">\(\boldsymbol{x}\)</span> but on approximations constructed using simulated observations.</p>
                <p>For summary statistics of the type <span class="math inline">\(\boldsymbol{n}_T^\textrm{obs}(D) : \mathcal{X}_D \subseteq \mathbb{R}^{d \times n } \longrightarrow \mathcal{Y}_D \subseteq \{0,1\}^{b}\)</span> the likelihood can be expressed as a product of independent Poisson count likelihoods as shown in Equation <a href="3-1-statistical-modelling.html#eq:poisson_multichannel">3.28</a>. Such likelihood can be evaluated for the observed data <span class="math inline">\(D\)</span> and specific parameters <span class="math inline">\(\boldsymbol{\theta}_R\)</span>, even in the case that <span class="math inline">\(\boldsymbol{\theta}\)</span> modifies the distribution of the mixture components <span class="math inline">\(p_j(\boldsymbol{x} | \boldsymbol{\theta})\)</span>, by forward approximating <span class="math inline">\(n^{\mathcal{C}_i}_j(\boldsymbol{\theta}_R)\)</span> (or alternatively <span class="math inline">\(\epsilon^{\mathcal{C}_i}_j(\boldsymbol{\theta}_R)\)</span>) using simulated observations for each process <span class="math inline">\(j\)</span> generated for <span class="math inline">\(\boldsymbol{\theta}_R\)</span>. This process would rapidly become computationally very demanding if it had to be repeated for each likelihood evaluation during the whole inference process. Re-weighting procedures such as those described in Equation <a href="3-1-statistical-modelling.html#eq:gen_level_reweighting">3.18</a> can often be applied to re-use already simulated events using <span class="math inline">\(\boldsymbol{\theta}_R\)</span> to model events corresponding to different values of the parameters <span class="math inline">\(\boldsymbol{\theta}_Q\)</span>.</p>
                <p>A more economical approach, commonly used in LHC analyses that use binned Poisson likelihoods based on the formalism introduced in Equation <a href="3-1-statistical-modelling.html#eq:poisson_multichannel">3.28</a>, is to parametrise the effect of varying parameters by interpolating between the values of the <span class="math inline">\(\epsilon^{\mathcal{C}_i}_j(\boldsymbol{\theta}_k)\)</span> (or directly <span class="math inline">\(n^{\mathcal{C}_i}_j(\boldsymbol{\theta}_k)\)</span>) for different values of <span class="math inline">\(k\)</span>. Such parametrisation allows the analytical approximation of the likelihood originated by Equation <a href="3-1-statistical-modelling.html#eq:poisson_multichannel">3.28</a>, and simplifies the computation of gradients with respect to the parameters. This is particularly relevant to model the effect of <em>nuisance parameters</em>, which are uncertain but not of direct interest, and have to be accounted for in the inference procedure; this issue will be discussed in Section <a href="3-1-statistical-modelling.html#sec:known_unknowns">3.1.4</a>. Different interpolation conventions exist <span class="citation">[<a href="references.html#ref-Cranmer:2015nia" role="doc-biblioref">90</a>]</span>, but they are normally based on the marginal one-dimensional interpolation between the effect of a single parameter <span class="math inline">\(\theta_i \in \boldsymbol{\theta}\)</span> at three equally spaced values (the nominal parameter values and the up/down variations). In that case the total effect on <span class="math inline">\(\epsilon^{\mathcal{C}_i}_j(\boldsymbol{\theta}_k)\)</span> is accounted by adding absolute shifts or multiplying marginal effects.</p>
                <p>Even assuming that the marginal description when a single parameter of interest varies is accurate, which is not ensured by the interpolation, and the effect of each parameter is factorised in <span class="math inline">\(p_j(\boldsymbol{x} | \boldsymbol{\theta})\)</span>, the integral definition of <span class="math inline">\(\epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_k)\)</span> from Equation <a href="3-1-statistical-modelling.html#eq:montecarlo_eff">3.13</a> does not ensure that the correlated effect of the variation of multiple <span class="math inline">\(\theta_i \in \boldsymbol{\theta}\)</span> is accurately modelled. This issue can be easily exemplified, considering the product of relative variations in the two parameter case <span class="math inline">\(\boldsymbol{\theta}_R = (\theta^R_0,\theta^R_1)\)</span>. Let us consider the expected value for the efficiency after a given selection <span class="math inline">\(\mathbb{1}_{\mathcal{C}_i}(\boldsymbol{x})\)</span>: <span id="eq:relative_var_integral"><span class="math display">\[
                \begin{aligned}
                \epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_R) &amp;=  \int
                \mathbb{1}_\mathcal{C}(\boldsymbol{x})
                p_j ( \boldsymbol{x}|\boldsymbol{\theta}_R ) d\boldsymbol{x} \\
                &amp;=
                \int \mathbb{1}_\mathcal{C}(\boldsymbol{x})
                p_j ( \boldsymbol{x}|\boldsymbol{\theta}_Q )
                \frac{p_j ( \boldsymbol{x}| (\theta^R_0,\theta^Q_1))}{p_j ( \boldsymbol{x}|\boldsymbol{\theta}_Q )}
                \frac{p_j ( \boldsymbol{x}| (\theta^Q_0,\theta^R_1))}{p_j ( \boldsymbol{x}|\boldsymbol{\theta}_Q )}
                 d\boldsymbol{x}
                \end{aligned}
                \qquad(3.31)\]</span></span> where <span class="math inline">\(\boldsymbol{\theta}_R\)</span> is the parameter point we want to simulate by interpolating around a nominal point <span class="math inline">\(\boldsymbol{\theta}_Q\)</span>. The last expression in Equation <a href="3-1-statistical-modelling.html#eq:relative_var_integral">3.31</a> is only correct when the effect of each parameter is independent, i.e. the underlying probability density function can be factorised as the product of independent factors. However, it becomes evident that the previous expression does not simplify: <span id="eq:relative_var_eff"><span class="math display">\[
                \epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_R) \neq
                \epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_Q)
                \frac{\epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_R)}{
                \epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_Q)}
                \frac{\epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_R)}{
                \epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta}_Q)}
                \qquad(3.32)\]</span></span> because the integral of the product of functions is not product of integrals, unless the volume of the selected region <span class="math inline">\(C\)</span> is infinitesimally small - an irrelevant case as it would correspond to null efficiencies. This effect also applies if additive variations are considered and can be more notable when more parameters are considered.</p>
                <p>The previously mentioned modelling issue, even though to the best of our knowledge has not been made explicit in the literature before, affects a multitude of analyses at the LHC, i.e. those that use <em>template interpolation</em>, as implemented in the standard statistical libraries used in particle physics experiments <span class="citation">[<a href="references.html#ref-Conway:2011in" role="doc-biblioref">91</a>], [<a href="references.html#ref-Cranmer:2012sba" role="doc-biblioref">92</a>]</span>. A possible solution would include doing a multi-dimensional interpolation, but it would naively require evaluating at least all 3-point combinatorial variations of the parameters, amounting to a minimum of <span class="math inline">\(3^p\)</span> evaluations of <span class="math inline">\(\epsilon^{\mathcal{C}_i}_j (\boldsymbol{\theta})\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters. If the effect of the parameters can be factored out in the joint distribution and the same simulated event set can be modified to describe each marginal variation, as reviewed around Equation <a href="3-1-statistical-modelling.html#eq:latent_reweighting">3.17</a>, the non-marginal terms can be estimated from the product of per-event marginal terms by considering the finite sum approximation of the last expression in Equation <a href="3-1-statistical-modelling.html#eq:relative_var_integral">3.31</a>, which would only require <span class="math inline">\((2p+1)\)</span> parameter variation evaluations. Alternatively, the basis of the approach presented in Chapter <a href="6-inference-aware-neural-optimisation.html#sec:inferno">6</a>, where the variation of the parameters and its derivatives are computed in place over the simulated observations by specifying the full computational graph, could also be used in analyses where the discussed assumption fails to realistically describe the data.</p>
                </div>
                </div>
                <div id="sec:known_unknowns" class="section level3">
                <h3><span class="header-section-number">3.1.4</span> Known Unknowns</h3>
                <p>So far we have assumed that the simulated observations can model the data and the only parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> that affect the generative model are those we are interested in carrying out inference on. However, simulated observations effectively depend on the modelling of the physical processes occurring in the proton-proton collisions and the detector, of which we only have an approximate description. Those mis-modelling effects have to be accounted in the inference procedure to obtain unbiased estimates, and are accounted by additional <em>nuisance parameters</em> in the statistical model when their effect is known and can be approximated. For cases where simulation does not provide the desired level of accuracy, the contribution from some of the mixture components can often be estimated from data directly, using what are referred to as <em>data-driven estimation</em> techniques.</p>
                <div id="sec:nuis_pars" class="section level4">
                <h4><span class="header-section-number">3.1.4.1</span> Nuisance Parameters</h4>
                <p>The general definition of nuisance parameters in a statistical model refers to all the uncertain parameters of the statistical model that are not of intermediate interest but have to be accounted for in the inference procedure. These parameters can include uncertain theoretical parameters (e.g. top quark mass or expected background rate), account for limitation on the experimentally measured parameterisations of certain phenomena (e.g. parton density functions uncertainties) or represent the accuracy limits of calibration between data and simulation. Nuisance parameters can also represent additional degrees of freedom of the model that cover for possible wrong assumptions or quantify imprecisions due to the limited number of simulated observations.</p>
                <p>Because the actual generative process for the experimental data is not known perfectly, the simulation-based model is extended with additional parameters that portray the possible variability on the distribution of the detector readouts. The formalism developed in the previous part of Section <a href="3-1-statistical-modelling.html#sec:stat_model">3.1</a> still applies, noting that the parameter vector <span class="math inline">\(\boldsymbol{\theta}=\{\boldsymbol{\theta}_\iota,\boldsymbol{\theta}_\nu\}\)</span> now includes both parameters of interest <span class="math inline">\(\boldsymbol{\theta}_\iota\)</span> and nuisance parameters <span class="math inline">\(\boldsymbol{\theta}_\nu\)</span>. While the effect of (usually theoretical) parameters of interest typically only affects the parton-level latent factor <span class="math inline">\(p(\boldsymbol{z}_\textrm{p} | \boldsymbol{\theta})\)</span>, some nuisance parameters account for possible mis-modelling in subsequent steps of the simulation thus can affect the other factors in Equation <a href="3-1-statistical-modelling.html#eq:factor_joint">3.9</a>.</p>
                <p>The effect of variation of nuisance parameters for any observable or summary statistic considered in a given analysis can be estimated by simulating again the affected observation with the chosen parameters - often prohibitively expensive - or by re-weighting already simulated observations as described in Equation <a href="3-1-statistical-modelling.html#eq:latent_reweighting">3.17</a> - which is much faster and reduces the statistical fluctuations between variations associated with the random sampling of the full latent space. Unprincipled modelling shortcuts, such as considering the additive or multiplicative effect of marginal efficiencies to account for combined effects, are also frequently used for count vector observables <span class="math inline">\(n^{\mathcal{C}_i}_j(\boldsymbol{\theta})\)</span>, as discussed in Section <a href="3-1-statistical-modelling.html#sec:synthetic_likelihood">3.1.3.4</a> together with possible solutions to some of the associated issues.</p>
                <p>The re-weighting approach from Equation <a href="3-1-statistical-modelling.html#eq:reweight_intractable">3.16</a> is extremely effective to model the effect of parameters in the conditional factor that deal with low-dimensional latent variables, such as <span class="math inline">\(p(\boldsymbol{z}_\textrm{p} | \boldsymbol{\theta})\)</span>, because the rest of the factors in the joint distribution simplify and we are left with a low-dimensionality density estimation problem (even analytically tractable in some cases). For conditional factors that deal with higher dimensional latent or observable spaces, such as <span class="math inline">\(p(\boldsymbol{z}_\textrm{d} |\boldsymbol{z}_\textrm{s}, \boldsymbol{\theta})\)</span> or <span class="math inline">\(p(\boldsymbol{x} |\boldsymbol{z}_\textrm{d}, \boldsymbol{\theta})\)</span>, the ratio can be very hard to estimate unless additional simplifications are possible. For those nuisance parameters, it is easier to consider the effect on the lower-dimensional summary statistic instead of the detector readout <span class="math inline">\(x\)</span>, because the ratio: <span id="eq:reweight_summary"><span class="math display">\[
                w(\boldsymbol{s}(\boldsymbol{x})) =
                \frac{p_R(\boldsymbol{s}(\boldsymbol{x})|\boldsymbol{\theta}_R)}{
                p_Q(\boldsymbol{s}(\boldsymbol{x})|\boldsymbol{\theta}_Q)}
                \qquad(3.33)\]</span></span> can be simpler to estimate through density estimation or approximately factorise if the summary statistic is chosen carefully. This fact motivates an alternative way to model the effect of some of the nuisance parameters, especially those related with the differences in the reconstructed objects observables between simulation and data after calibration. Let us consider the case where summary statistics <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x}) : \mathcal{X} \subseteq \mathbb{R}^{d} \longrightarrow \mathcal{Y}_\textrm{sum} \subseteq \mathbb{R}^{b}\)</span> are effectively a function of the reconstructed objects and its properties <span class="math inline">\(\boldsymbol{y}_\textrm{reco} \in \mathcal{Y}_\textrm{reco}\)</span>, which can be schematically represented by the following function composition chain: <span id="eq:composition_summary"><span class="math display">\[
                \mathcal{X} \overset{g}{\longrightarrow} \mathcal{Y}_\textrm{reco} \overset{h}{\longrightarrow} \mathcal{Y}_\textrm{sum}
                \qquad(3.34)\]</span></span> where <span class="math inline">\(\boldsymbol{y}_\textrm{reco} = g(\boldsymbol{x})\)</span> and <span class="math inline">\(\boldsymbol{y}_\textrm{sum} = h(\boldsymbol{y}_\textrm{reco})\)</span>. This compositional approach can be extended to include also event selection at trigger or analysis level, or other intermediate summaries of <span class="math inline">\(\boldsymbol{x}\)</span> complementary to reconstruction, as part of the definition of the summary statistic <span class="math inline">\(\boldsymbol{s}(\boldsymbol{x})\)</span>. In all cases where <span class="math inline">\(s(\boldsymbol{x})\)</span> is a deterministic function, all differences between simulated observations and data in any expected observables originate from the differences between the simulation-based generative definition of <span class="math inline">\(p(\boldsymbol{x} | \boldsymbol{\theta})\)</span> and the true unknown generative process <span class="math inline">\(p_\textrm{true}(\boldsymbol{x})\)</span>. While the task of evaluating and parametrising these differences directly by studying the raw detector output is quite convoluted, the differences can be corrected and their uncertainty assessed for the lower-dimensional intermediate states of the composition chain depicted in Equation <a href="3-1-statistical-modelling.html#eq:composition_summary">3.34</a>.</p>
                <p>For example, if the momenta of a certain subset of the reconstructed objects <span class="math inline">\(\boldsymbol{y}_\textrm{reco}\)</span> statistically differ between experimental data and the simulated observations, based on a subset of the data that is assumed to be well-modelled, the momenta of simulated observations can be corrected to better model the data. The statistical accuracy of such procedure due to the different factors leads to a set of nuisance parameters that describe the limit of the mentioned calibration as a function of the value of <span class="math inline">\(\boldsymbol{y}_\textrm{reco}\)</span>. The effect of these type of nuisance parameters often be modelled in the simulation by using a function of the simulated intermediate outputs, e.g. in the case of reconstructed objects: <span id="eq:exp_rep"><span class="math display">\[
                \mathop{\mathbb{E}}_{\boldsymbol{x} \sim  p( \boldsymbol{x}| \boldsymbol{\theta} )}
                \left [ \boldsymbol{s}(\boldsymbol{x}) \right ] =
                \mathop{\mathbb{E}}_{\boldsymbol{y}_\textrm{reco} \sim  p( \boldsymbol{y}_\textrm{reco}| \boldsymbol{\theta}_o )}
                \left [ h(r(\boldsymbol{y}_\textrm{reco}, \boldsymbol{\theta}_\rho))\right ]
                \qquad(3.35)\]</span></span> so <span class="math inline">\(p( \boldsymbol{x}| \boldsymbol{\theta} )\)</span> can be approximated by computing observables after applying the re-parametrisation <span class="math inline">\(r(\boldsymbol{y}_\textrm{reco}, \boldsymbol{\theta}_\rho)\)</span> to the simulated observations, where <span class="math inline">\(\boldsymbol{\theta}_\rho\)</span> is the vector of parameters representing the different uncertainty factors.</p>
                <p>In general, the effects of all relevant nuisance parameters can be modelled by a combination of simulated observations re-weighting by <span class="math inline">\(w(\boldsymbol{x}_i,\boldsymbol{z}_i | \boldsymbol{\theta}_w )\)</span> and transformations of intermediate simulated observations <span class="math inline">\(\boldsymbol{y}_\textrm{new} = r(\boldsymbol{y}_\textrm{sim},\boldsymbol{z}_i | \boldsymbol{\theta}_\rho)\)</span>. The former is based on importance sampling <span class="citation">[<a href="references.html#ref-mcbook" role="doc-biblioref">93</a>]</span> to estimate the properties of a different distribution than the one sampled originally from, while the latter assumes that the mis-modelling can be accounted by a parametrisation of the simulated intermediate observables. If the functions <span class="math inline">\(w(\boldsymbol{x}_i,\boldsymbol{z}_i | \boldsymbol{\theta}_w )\)</span> and <span class="math inline">\(r(\boldsymbol{y}_\textrm{sim},\boldsymbol{z}_i | \boldsymbol{\theta}_\rho)\)</span> are differentiable or can be approximated by differentiable functions, the gradient (and higher order derivatives) with respect to the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> of any expectation value can be very efficiently approximated. This can be very useful for statistical inference (e.g. likelihood maximisation), while it has not been used so far in LHC analysis to our knowledge. This is one of the core concepts of the technique to construct summary statistics presented in Chapter <a href="6-inference-aware-neural-optimisation.html#sec:inferno">6</a>.</p>
                <p>The inference results of a given analysis depend strongly on the assumptions implicit in the statistical model. The determination, assessment and practical definition of the effect of nuisance parameters that are relevant for a given analysis is one the most challenging yet important aspects in experimental particle physics at the LHC. When nuisance parameters are quantitatively taken into account in the statistical model, they lead to an increase of the uncertainty on the parameters of interest and larger interval estimates (or exclusion limits) on the parameters of interest. The choice of summary statistics may also affect significantly subsequent inference, and while nuisance parameters are usually qualitatively considered when building simple summary statistics by physics-inspired combinations of reconstructed variables, they are not regarded at all when the automatic multi-variate techniques described in Chapter <a href="4-machine-learning-in-high-energy-physics.html#sec:machine_learning">4</a> are applied to construct complex non-linear observables. This issue is addressed by the method proposed in Chapter <a href="6-inference-aware-neural-optimisation.html#sec:inferno">6</a>.</p>
                </div>
                <div id="sec:data_driven" class="section level4">
                <h4><span class="header-section-number">3.1.4.2</span> Data-Driven Estimation</h4>
                <p>For some fundamental processes, the generative modelling provided by simulated observations might not be accurate enough for the purposes of a given LHC analysis. In a subset of those cases, the simulated observations can be calibrated to better describe the observations in well-modelled data regions, as mentioned in the previous section. However, if the description of the summary statistics considered in the analysis provided by the simulated observations from process <span class="math inline">\(j\)</span> is substandard, e.g. the number of simulated observations that could be realistically simulated is not sufficient, then the contribution from the mentioned mixture component might have to be estimated from experimental observations directly.</p>
                <p>The actual procedure used for modelling the contribution for a given mixture component <span class="math inline">\(j\)</span> from data depend on the specifics of the process as well the details of the analysis, but often includes some re-weighting factor obtained from simulated observations or additional experimental observations with an orthogonal selection criterion. Such data-driven estimation techniques are often used for the background processes, but are hard to combine with the non-linear summary statistics reconstructed by machine learning techniques such as those described in Chapter <a href="4-machine-learning-in-high-energy-physics.html#sec:machine_learning">4</a>. In the CMS analysis presented in Chapter <a href="5-search-for-anomalous-higgs-pair-production-with-cms.html#sec:higgs_pair">5</a>, we describe and utilise a fully data-driven background estimation technique fine-tuned for the modelling of the QCD-based multiple jet background for the search of Higgs pair production decaying to four b-quarks.</p>
                </div>
                </div>
                </div>
                <div class="footnotes">
                <hr><ol start="6"><li id="fn3"><p>In this work, synthetic likelihood will be used to refer to likelihoods that are not based on the probability distribution function of the generative model, but on non-parametric approximations using low-dimensional summaries of the data.<a href="3-1-statistical-modelling.html#fnref3" class="footnote-back">↩</a></p></li>
                <li id="fn4"><p>Here categorical distribution refers to the special case of the multinomial distribution were the number of trials is one.<a href="3-1-statistical-modelling.html#fnref4" class="footnote-back">↩</a></p></li>
                <li id="fn5"><p>The term <em>group/type of interactions</em> here generally refers to a set of processes that could be generatively modelled independently, not to quantum mechanical amplitudes or intensities of a process. For example, each group can correspond to a group of processes with a given final state <span class="math inline">\(pp \rightarrow X\)</span> which could be modelled by sampling its differential cross section from Equation <a href="1-3-phenomenology-of-proton-collisions.html#eq:qcd_factorisation">1.32</a> followed by parton showering and detector simulation. The group category is a latent/hidden variable for each event, i.e. it is not observed.<a href="3-1-statistical-modelling.html#fnref5" class="footnote-back">↩</a></p></li>
                <li id="fn6"><p>In particle physics, a resonance is a peak around a certain energy in the differential cross section associated with the production of subatomic particles.<a href="3-1-statistical-modelling.html#fnref6" class="footnote-back">↩</a></p></li>
                </ol></div>
                
                              </section></div>
          </div>
        </div>
        
        
      <a href="3-2-statistical-inference.html" class="navigation navigation-next" aria-label="Next page">
                <i class="fa fa-angle-right"></i></a><a href="3-statistical-modelling-and-inference-at-the-lhc.html" class="navigation navigation-prev" aria-label="Previous page">
                <i class="fa fa-angle-left"></i></a></div>
    </div>
    

    <script src="libs/gitbook/js/app.min.js"></script><script src="libs/gitbook/js/lunr.js"></script><script src="libs/gitbook/js/plugin-search.js"></script><script src="libs/gitbook/js/plugin-sharing.js"></script><script src="libs/gitbook/js/plugin-fontsettings.js"></script><script src="libs/gitbook/js/plugin-bookdown.js"></script><script src="libs/gitbook/js/jquery.highlight.js"></script><script>
      gitbook.require(["gitbook"], function(gitbook) {
        gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook","twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"history": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "none"
}
});
});
    </script><script>
      (function () {
        var script = document.createElement("script");
        script.type = "text/javascript";
        var src = "true";
        if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
        if (location.protocol !== "file:" && /^https?:/.test(src))
          src = src.replace(/^https?:/, '');
        script.src = src;
        document.getElementsByTagName("head")[0].appendChild(script);
      })();
    </script><script src="https://hypothes.is/embed.js" async></script><link href="css/annotator.css" rel="stylesheet"></body></html>