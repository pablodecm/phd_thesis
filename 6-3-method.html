<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-71094563-2"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-71094563-2');
    </script><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Statistical Learning and Inference at Particle Collider Experiments</title><meta name="description" content="Statistical Learning and Inference at Particle Collider Experiments"><meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7"><meta property="og:title" content="Statistical Learning and Inference at Particle Collider Experiments"><meta property="og:type" content="book"><meta name="github-repo" content="pablodecm/phd_thesis"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Statistical Learning and Inference at Particle Collider Experiments"><meta name="author" content="Pablo de Castro Manzano"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><script src="libs/jquery/jquery.min.js"></script><link href="libs/gitbook/css/style.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-table.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-search.css" rel="stylesheet"><link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet"><link href="css/style.css" rel="stylesheet"><link href="css/toc.css" rel="stylesheet"></head><body>

    
    <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

      <div class="book-summary">
        <nav role="navigation"><ul class="summary"><li>
              <a href="./">PhD Thesis - Pablo de Castro</a>
            </li>
            <li class="divider">
            <li class="chapter" data-level="" data-path="abstract.html">
              <a href="abstract.html"><i class="fa fa-check"></i> Abstract</a>
            </li>
            <li class="chapter" data-level="" data-path="preface.html">
              <a href="preface.html"><i class="fa fa-check"></i> Preface</a>
            </li>
            <li class="chapter" data-level="" data-path="acknowledgements.html">
              <a href="acknowledgements.html"><i class="fa fa-check"></i> Acknowledgements</a>
            </li>
            <li class="chapter" data-level="" data-path="introduction.html">
              <a href="introduction.html"><i class="fa fa-check"></i> Introduction</a>
            </li>
            <li class="chapter" data-level="1" data-path="1-theory-of-fundamental-interactions.html">
              <a href="1-theory-of-fundamental-interactions.html"><i class="fa fa-check"></i><b>1</b> Theory of Fundamental Interactions</a>
              <ul><li class="chapter" data-level="1.1" data-path="1-1-the-standard-model.html">
                  <a href="1-1-the-standard-model.html"><i class="fa fa-check"></i><b>1.1</b> The Standard Model</a>
                  <ul><li class="chapter" data-level="1.1.1" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:qft_basics"><i class="fa fa-check"></i><b>1.1.1</b> Essentials of Quantum Field Theory</a>
                    </li>
                    <li class="chapter" data-level="1.1.2" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:qcd_detail"><i class="fa fa-check"></i><b>1.1.2</b> Quantum Chromodynamics</a>
                    </li>
                    <li class="chapter" data-level="1.1.3" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:ew_detail"><i class="fa fa-check"></i><b>1.1.3</b> Electroweak Interactions</a>
                    </li>
                    <li class="chapter" data-level="1.1.4" data-path="1-1-the-standard-model.html">
                      <a href="1-1-the-standard-model.html#sec:ewsb_higgs"><i class="fa fa-check"></i><b>1.1.4</b> Symmetry Breaking and the Higgs Boson</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="1.2" data-path="1-2-beyond-the-standard-model.html">
                  <a href="1-2-beyond-the-standard-model.html"><i class="fa fa-check"></i><b>1.2</b> Beyond the Standard Model</a>
                  <ul><li class="chapter" data-level="1.2.1" data-path="1-2-beyond-the-standard-model.html">
                      <a href="1-2-beyond-the-standard-model.html#known-limitations"><i class="fa fa-check"></i><b>1.2.1</b> Known Limitations</a>
                    </li>
                    <li class="chapter" data-level="1.2.2" data-path="1-2-beyond-the-standard-model.html">
                      <a href="1-2-beyond-the-standard-model.html#sec:possible_ext"><i class="fa fa-check"></i><b>1.2.2</b> Possible Extensions</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="1.3" data-path="1-3-phenomenology-of-proton-collisions.html">
                  <a href="1-3-phenomenology-of-proton-collisions.html"><i class="fa fa-check"></i><b>1.3</b> Phenomenology of Proton Collisions</a>
                  <ul><li class="chapter" data-level="1.3.1" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:main_obs"><i class="fa fa-check"></i><b>1.3.1</b> Main Observables</a>
                    </li>
                    <li class="chapter" data-level="1.3.2" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:pdfs"><i class="fa fa-check"></i><b>1.3.2</b> Parton Distribution Functions</a>
                    </li>
                    <li class="chapter" data-level="1.3.3" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:factorisation"><i class="fa fa-check"></i><b>1.3.3</b> Factorisation and Generation of Hard Processes</a>
                    </li>
                    <li class="chapter" data-level="1.3.4" data-path="1-3-phenomenology-of-proton-collisions.html">
                      <a href="1-3-phenomenology-of-proton-collisions.html#sec:parton_showers"><i class="fa fa-check"></i><b>1.3.4</b> Hadronization and Parton Showers</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="2" data-path="2-experiments-at-particle-colliders.html">
              <a href="2-experiments-at-particle-colliders.html"><i class="fa fa-check"></i><b>2</b> Experiments at Particle Colliders</a>
              <ul><li class="chapter" data-level="2.1" data-path="2-1-the-large-hadron-collider.html">
                  <a href="2-1-the-large-hadron-collider.html"><i class="fa fa-check"></i><b>2.1</b> The Large Hadron Collider</a>
                  <ul><li class="chapter" data-level="2.1.1" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#injection-and-acceleration-chain"><i class="fa fa-check"></i><b>2.1.1</b> Injection and Acceleration Chain</a>
                    </li>
                    <li class="chapter" data-level="2.1.2" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#sec:op_pars"><i class="fa fa-check"></i><b>2.1.2</b> Operation Parameters</a>
                    </li>
                    <li class="chapter" data-level="2.1.3" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#sec:pile_up"><i class="fa fa-check"></i><b>2.1.3</b> Multiple Hadron Interactions</a>
                    </li>
                    <li class="chapter" data-level="2.1.4" data-path="2-1-the-large-hadron-collider.html">
                      <a href="2-1-the-large-hadron-collider.html#sec:lhc_experiments"><i class="fa fa-check"></i><b>2.1.4</b> Experiments</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="2.2" data-path="2-2-the-compact-muon-solenoid.html">
                  <a href="2-2-the-compact-muon-solenoid.html"><i class="fa fa-check"></i><b>2.2</b> The Compact Muon Solenoid</a>
                  <ul><li class="chapter" data-level="2.2.1" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:exp_geom"><i class="fa fa-check"></i><b>2.2.1</b> Experimental Geometry</a>
                    </li>
                    <li class="chapter" data-level="2.2.2" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_magnet"><i class="fa fa-check"></i><b>2.2.2</b> Magnet</a>
                    </li>
                    <li class="chapter" data-level="2.2.3" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_tracking"><i class="fa fa-check"></i><b>2.2.3</b> Tracking System</a>
                    </li>
                    <li class="chapter" data-level="2.2.4" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_ecal"><i class="fa fa-check"></i><b>2.2.4</b> Electromagnetic Calorimeter</a>
                    </li>
                    <li class="chapter" data-level="2.2.5" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_hcal"><i class="fa fa-check"></i><b>2.2.5</b> Hadronic Calorimeter</a>
                    </li>
                    <li class="chapter" data-level="2.2.6" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:cms_muon"><i class="fa fa-check"></i><b>2.2.6</b> Muon System</a>
                    </li>
                    <li class="chapter" data-level="2.2.7" data-path="2-2-the-compact-muon-solenoid.html">
                      <a href="2-2-the-compact-muon-solenoid.html#sec:trigger"><i class="fa fa-check"></i><b>2.2.7</b> Trigger and Data Acquisition</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="2.3" data-path="2-3-event-simulation-and-reconstruction.html">
                  <a href="2-3-event-simulation-and-reconstruction.html"><i class="fa fa-check"></i><b>2.3</b> Event Simulation and Reconstruction</a>
                  <ul><li class="chapter" data-level="2.3.1" data-path="2-3-event-simulation-and-reconstruction.html">
                      <a href="2-3-event-simulation-and-reconstruction.html#sec:gen_view"><i class="fa fa-check"></i><b>2.3.1</b> A Generative View</a>
                    </li>
                    <li class="chapter" data-level="2.3.2" data-path="2-3-event-simulation-and-reconstruction.html">
                      <a href="2-3-event-simulation-and-reconstruction.html#sec:detector_simulation"><i class="fa fa-check"></i><b>2.3.2</b> Detector Simulation</a>
                    </li>
                    <li class="chapter" data-level="2.3.3" data-path="2-3-event-simulation-and-reconstruction.html">
                      <a href="2-3-event-simulation-and-reconstruction.html#sec:event_reco"><i class="fa fa-check"></i><b>2.3.3</b> Event Reconstruction</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="3" data-path="3-statistical-modelling-and-inference-at-the-lhc.html">
              <a href="3-statistical-modelling-and-inference-at-the-lhc.html"><i class="fa fa-check"></i><b>3</b> Statistical Modelling and Inference at the LHC</a>
              <ul><li class="chapter" data-level="3.1" data-path="3-1-statistical-modelling.html">
                  <a href="3-1-statistical-modelling.html"><i class="fa fa-check"></i><b>3.1</b> Statistical Modelling</a>
                  <ul><li class="chapter" data-level="3.1.1" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#sec:model_overview"><i class="fa fa-check"></i><b>3.1.1</b> Overview</a>
                    </li>
                    <li class="chapter" data-level="3.1.2" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#simulation-as-generative-modelling"><i class="fa fa-check"></i><b>3.1.2</b> Simulation as Generative Modelling</a>
                    </li>
                    <li class="chapter" data-level="3.1.3" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#sec:dim_reduction"><i class="fa fa-check"></i><b>3.1.3</b> Dimensionality Reduction</a>
                    </li>
                    <li class="chapter" data-level="3.1.4" data-path="3-1-statistical-modelling.html">
                      <a href="3-1-statistical-modelling.html#sec:known_unknowns"><i class="fa fa-check"></i><b>3.1.4</b> Known Unknowns</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="3.2" data-path="3-2-statistical-inference.html">
                  <a href="3-2-statistical-inference.html"><i class="fa fa-check"></i><b>3.2</b> Statistical Inference</a>
                  <ul><li class="chapter" data-level="3.2.1" data-path="3-2-statistical-inference.html">
                      <a href="3-2-statistical-inference.html#sec:likelihood-free"><i class="fa fa-check"></i><b>3.2.1</b> Likelihood-Free Inference</a>
                    </li>
                    <li class="chapter" data-level="3.2.2" data-path="3-2-statistical-inference.html">
                      <a href="3-2-statistical-inference.html#sec:hypo_test"><i class="fa fa-check"></i><b>3.2.2</b> Hypothesis Testing</a>
                    </li>
                    <li class="chapter" data-level="3.2.3" data-path="3-2-statistical-inference.html">
                      <a href="3-2-statistical-inference.html#sec:param_est"><i class="fa fa-check"></i><b>3.2.3</b> Parameter Estimation</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="4" data-path="4-machine-learning-in-high-energy-physics.html">
              <a href="4-machine-learning-in-high-energy-physics.html"><i class="fa fa-check"></i><b>4</b> Machine Learning in High-Energy Physics</a>
              <ul><li class="chapter" data-level="4.1" data-path="4-1-problem-description.html">
                  <a href="4-1-problem-description.html"><i class="fa fa-check"></i><b>4.1</b> Problem Description</a>
                  <ul><li class="chapter" data-level="4.1.1" data-path="4-1-problem-description.html">
                      <a href="4-1-problem-description.html#sec:supervised"><i class="fa fa-check"></i><b>4.1.1</b> Probabilistic Classification and Regression</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="4.2" data-path="4-2-machine-learning-techniques.html">
                  <a href="4-2-machine-learning-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Machine Learning Techniques</a>
                  <ul><li class="chapter" data-level="4.2.1" data-path="4-2-machine-learning-techniques.html">
                      <a href="4-2-machine-learning-techniques.html#sec:boosted_decision_trees"><i class="fa fa-check"></i><b>4.2.1</b> Boosted Decision Trees</a>
                    </li>
                    <li class="chapter" data-level="4.2.2" data-path="4-2-machine-learning-techniques.html">
                      <a href="4-2-machine-learning-techniques.html#sec:ann"><i class="fa fa-check"></i><b>4.2.2</b> Artificial Neural Networks</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="4.3" data-path="4-3-applications-in-high-energy-physics.html">
                  <a href="4-3-applications-in-high-energy-physics.html"><i class="fa fa-check"></i><b>4.3</b> Applications in High Energy Physics</a>
                  <ul><li class="chapter" data-level="4.3.1" data-path="4-3-applications-in-high-energy-physics.html">
                      <a href="4-3-applications-in-high-energy-physics.html#sec:sig_vs_bkg"><i class="fa fa-check"></i><b>4.3.1</b> Signal vs Background Classification</a>
                    </li>
                    <li class="chapter" data-level="4.3.2" data-path="4-3-applications-in-high-energy-physics.html">
                      <a href="4-3-applications-in-high-energy-physics.html#sec:particle_id_reg"><i class="fa fa-check"></i><b>4.3.2</b> Particle Identification and Regression</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="5" data-path="5-search-for-anomalous-higgs-pair-production-with-cms.html">
              <a href="5-search-for-anomalous-higgs-pair-production-with-cms.html"><i class="fa fa-check"></i><b>5</b> Search for Anomalous Higgs Pair Production with CMS</a>
              <ul><li class="chapter" data-level="5.1" data-path="5-1-introduction.html">
                  <a href="5-1-introduction.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
                </li>
                <li class="chapter" data-level="5.2" data-path="5-2-higgs-pair-production-and-anomalous-couplings.html">
                  <a href="5-2-higgs-pair-production-and-anomalous-couplings.html"><i class="fa fa-check"></i><b>5.2</b> Higgs Pair Production and Anomalous Couplings</a>
                </li>
                <li class="chapter" data-level="5.3" data-path="5-3-analysis-strategy.html">
                  <a href="5-3-analysis-strategy.html"><i class="fa fa-check"></i><b>5.3</b> Analysis Strategy</a>
                </li>
                <li class="chapter" data-level="5.4" data-path="5-4-trigger-and-datasets.html">
                  <a href="5-4-trigger-and-datasets.html"><i class="fa fa-check"></i><b>5.4</b> Trigger and Datasets</a>
                </li>
                <li class="chapter" data-level="5.5" data-path="5-5-event-selection.html">
                  <a href="5-5-event-selection.html"><i class="fa fa-check"></i><b>5.5</b> Event Selection</a>
                </li>
                <li class="chapter" data-level="5.6" data-path="5-6-data-driven-background-estimation.html">
                  <a href="5-6-data-driven-background-estimation.html"><i class="fa fa-check"></i><b>5.6</b> Data-Driven Background Estimation</a>
                  <ul><li class="chapter" data-level="5.6.1" data-path="5-6-data-driven-background-estimation.html">
                      <a href="5-6-data-driven-background-estimation.html#sec:hem_mixing"><i class="fa fa-check"></i><b>5.6.1</b> Hemisphere Mixing</a>
                    </li>
                    <li class="chapter" data-level="5.6.2" data-path="5-6-data-driven-background-estimation.html">
                      <a href="5-6-data-driven-background-estimation.html#sec:bkg_validation"><i class="fa fa-check"></i><b>5.6.2</b> Background Validation</a>
                    </li>
                  </ul></li>
                <li class="chapter" data-level="5.7" data-path="5-7-systematic-uncertainties.html">
                  <a href="5-7-systematic-uncertainties.html"><i class="fa fa-check"></i><b>5.7</b> Systematic Uncertainties</a>
                </li>
                <li class="chapter" data-level="5.8" data-path="5-8-analysis-results.html">
                  <a href="5-8-analysis-results.html"><i class="fa fa-check"></i><b>5.8</b> Analysis Results</a>
                </li>
                <li class="chapter" data-level="5.9" data-path="5-9-combination-with-other-decay-channels.html">
                  <a href="5-9-combination-with-other-decay-channels.html"><i class="fa fa-check"></i><b>5.9</b> Combination with Other Decay Channels</a>
                </li>
              </ul></li>
            <li class="chapter" data-level="6" data-path="6-inference-aware-neural-optimisation.html">
              <a href="6-inference-aware-neural-optimisation.html"><i class="fa fa-check"></i><b>6</b> Inference-Aware Neural Optimisation</a>
              <ul><li class="chapter" data-level="6.1" data-path="6-1-introduction.html">
                  <a href="6-1-introduction.html"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
                </li>
                <li class="chapter" data-level="6.2" data-path="6-2-problem-statement.html">
                  <a href="6-2-problem-statement.html"><i class="fa fa-check"></i><b>6.2</b> Problem Statement</a>
                </li>
                <li class="chapter" data-level="6.3" data-path="6-3-method.html">
                  <a href="6-3-method.html"><i class="fa fa-check"></i><b>6.3</b> Method</a>
                </li>
                <li class="chapter" data-level="6.4" data-path="6-4-related-work.html">
                  <a href="6-4-related-work.html"><i class="fa fa-check"></i><b>6.4</b> Related Work</a>
                </li>
                <li class="chapter" data-level="6.5" data-path="6-5-experiments.html">
                  <a href="6-5-experiments.html"><i class="fa fa-check"></i><b>6.5</b> Experiments</a>
                  <ul><li class="chapter" data-level="6.5.1" data-path="6-5-experiments.html">
                      <a href="6-5-experiments.html#sec:synthetic_mixture"><i class="fa fa-check"></i><b>6.5.1</b> 3D Synthetic Mixture</a>
                    </li>
                  </ul></li>
              </ul></li>
            <li class="chapter" data-level="7" data-path="7-conclusions-and-prospects.html">
              <a href="7-conclusions-and-prospects.html"><i class="fa fa-check"></i><b>7</b> Conclusions and Prospects</a>
            </li>
            <li class="chapter" data-level="" data-path="references.html">
              <a href="references.html"><i class="fa fa-check"></i> References</a>
            </li>
          </ul></nav></div>

      <div class="book-body">
        <div class="body-inner">
          <div class="book-header" role="navigation">
            <h1>
              <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Inference at Particle Collider Experiments</a>
            </h1>
          </div>

          <div class="page-wrapper" tabindex="-1" role="main">
            <div class="page-inner">

              <section class="normal" id="section-"><div id="sec:method" class="section level2">
                <h2><span class="header-section-number">6.3</span> Method</h2>
                <p>In this section, a machine learning technique to learn non-linear sample summary statistics is described in detail. The method seeks to minimise the expected variance of the parameters of interest obtained via a non-parametric simulation-based synthetic likelihood. A graphical description of the technique is depicted on Fig. <a href="6-3-method.html#fig:diagram">6.1</a>. The parameters of a neural network are optimised by stochastic gradient descent within an automatic differentiation framework, where the considered loss function accounts for the details of the statistical model as well as the expected effect of nuisance parameters.</p>
                <div class="figure">
                <img src="gfx/106_chapter_6/figure1.svg" alt="Figure 6.1: Learning inference-aware summary statistics (see text for details)." id="fig:diagram" class="vector"><p class="caption">Figure 6.1: Learning inference-aware summary statistics (see text for details).</p>
                </div>
                <p>The family of summary statistics <span class="math inline">\(\boldsymbol{s}(D)\)</span> considered in this work is based on a neural network model applied to each dataset observation <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x}; \boldsymbol{\phi}) : \mathcal{X} \subseteq \mathbb{R}^{d} \rightarrow \mathcal{Y} \subseteq \mathbb{R}^{b}\)</span>, whose parameters <span class="math inline">\(\boldsymbol{\phi}\)</span> will be learned during training by means of stochastic gradient descent, as will be discussed later. Therefore, using set-builder notation the considered family of summary statistics considered can be denoted as: <span id="eq:summary"><span class="math display">\[
                \boldsymbol{s} (D, \boldsymbol{\phi})
                 = \boldsymbol{s} \left ( \: \{ \:  \boldsymbol{f}(\boldsymbol{x}_i; \boldsymbol{\phi}) \:
                  | \: \forall \: \boldsymbol{x}_i \in D \: \} \: \right )
                \qquad(6.3)\]</span></span> where <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x}_i; \boldsymbol{\phi})\)</span> will reduce the dimensionality from the input observations space <span class="math inline">\(\mathcal{X}\)</span> to a lower-dimensional space <span class="math inline">\(\mathcal{Y}\)</span>. The next step is to map observation outputs to a dataset summary statistic, which will in turn be calibrated and optimised via a non-parametric likelihood <span class="math inline">\(\mathcal{L}(D; \boldsymbol{\theta},\boldsymbol{\phi})\)</span> created using a set of simulated observations <span class="math inline">\(G_s= \{\boldsymbol{x}_0,...,\boldsymbol{x}_g\}\)</span>, generated at a certain instantiation of the simulator parameters <span class="math inline">\(\boldsymbol{\theta}_s\)</span>.</p>
                <p>In experimental high energy physics experiments, which are the scientific context that initially motivated this work, histograms of observation counts are the most commonly used non-parametric density estimator because the resulting likelihoods can be expressed as the product of Poisson factors, one for each of the considered bins. A naive sample summary statistic can be built from the output of the neural network by simply assigning each observation <span class="math inline">\(\boldsymbol{x}\)</span> to a bin corresponding to the cardinality of the maximum element of <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x}; \boldsymbol{\phi})\)</span>, so each element of the sample summary will correspond to the following sum: <span id="eq:argmax"><span class="math display">\[
                s_i(D;\boldsymbol{\phi})=\sum_{\boldsymbol{x} \in D}
                \begin{cases}
                      1 &amp; i = {argmax}_{j=\{0,...,b\}}
                        (f_j(\boldsymbol{x}; \boldsymbol{\phi})) \\
                      0 &amp; i \neq {argmax}_{j=\{0,...,b\}}
                        (f_j(\boldsymbol{x}; \boldsymbol{\phi})) \\
                   \end{cases}
                \qquad(6.4)\]</span></span> which can in turn be used to build the following likelihood, where the expectation for each bin is taken from the simulated sample <span class="math inline">\(G_s\)</span>: <span id="eq:likelihood"><span class="math display">\[
                \mathcal{L}(D; \boldsymbol{\theta},\boldsymbol{\phi})=\prod_{i=0 }^b
                             \textrm{Pois} \left ( s_i (D; \boldsymbol{\phi}) \:  |
                             \: \left ( \frac{n}{g} \right ) s_i (G_s;\boldsymbol{\phi}) \right )
                \qquad(6.5)\]</span></span> where the <span class="math inline">\(n/g\)</span> factor accounts for the different number of observations in the simulated samples. In cases where the number of observations is itself a random variable providing information about the parameters of interest, or where the simulated observations are weighted, the choice of normalisation of <span class="math inline">\(\mathcal{L}\)</span> may be slightly more involved and problem specific, but nevertheless amenable. Note the relation between the summary statistics and likelihoods defined in this section and those discussed in Section <a href="3-1-statistical-modelling.html#sec:dim_reduction">3.1.3</a>.</p>
                <p>In the above construction, the chosen family of summary statistics is not differentiable due to the <span class="math inline">\(argmax\)</span> operator, so gradient-based updates for the parameters cannot be computed. To work around this problem, a differentiable approximation <span class="math inline">\(\hat{\boldsymbol{s}}(D ; \boldsymbol{\phi})\)</span> is considered. This function is defined by means of a <span class="math inline">\(softmax\)</span> operator: <span id="eq:soft_summary"><span class="math display">\[
                \hat{s}_i(D;\boldsymbol{\phi})=\sum_{x \in D}
                  \frac{e^{f_i(\boldsymbol{x}; \boldsymbol{\phi})/\tau}}
                  {\sum_{j=0}^{b} e^{f_j(\boldsymbol{x}; \boldsymbol{\phi})/\tau}}
                \qquad(6.6)\]</span></span> where the temperature hyper-parameter <span class="math inline">\(\tau\)</span> will regulate the softness of the operator. In the limit of <span class="math inline">\(\tau \rightarrow 0^{+}\)</span>, the probability of the largest component will tend to 1 while others to 0, and therefore <span class="math inline">\(\hat{\boldsymbol{s}}(D ; \boldsymbol{\phi}) \rightarrow \boldsymbol{s}(D; \boldsymbol{\phi})\)</span>. Similarly, let us denote by <span class="math inline">\(\hat{\mathcal{L}}(D; \boldsymbol{\theta}, \boldsymbol{\phi})\)</span> the differentiable approximation of the non-parametric likelihood obtained by substituting <span class="math inline">\(\boldsymbol{s}(D ; \boldsymbol{\phi})\)</span> with <span class="math inline">\(\hat{\boldsymbol{s}}(D ; \boldsymbol{\phi})\)</span>. Instead of using the observed data <span class="math inline">\(D\)</span>, the value of <span class="math inline">\(\hat{\mathcal{L}}\)</span> may be computed when the observation for each bin is equal to its corresponding expectation based on the simulated sample <span class="math inline">\(G_s\)</span>, which is commonly denoted as the Asimov likelihood <span class="citation">[<a href="references.html#ref-Cowan:2010js" role="doc-biblioref">100</a>]</span> <span class="math inline">\(\hat{\mathcal{L}}_A\)</span>: <span id="eq:likelihood_asimov"><span class="math display">\[
                \hat{\mathcal{L}}_A(\boldsymbol{\theta}; \boldsymbol{\phi})=\prod_{i=0 }^b
                             \textrm{Pois} \left ( \left ( \frac{n}{g} \right )
                            \hat{s}_i (G_s;\boldsymbol{\phi}) \:  | \: \left ( \frac{n}{g} \right )
                             \hat{s}_i (G_s;\boldsymbol{\phi}) \right )
                \qquad(6.7)\]</span></span> for which it can be easily proven that <span class="math inline">\(argmax_{\boldsymbol{\theta} \in \mathcal{\theta}} (\hat{\mathcal{L}}_A( \boldsymbol{\theta; \boldsymbol{\phi}})) = \boldsymbol{\theta}_s\)</span>, so the maximum likelihood estimator (MLE) for the Asimov likelihood is the parameter vector <span class="math inline">\(\boldsymbol{\theta}_s\)</span> used to generate the simulated dataset <span class="math inline">\(G_s\)</span>. In Bayesian terms, if the prior over the parameters is flat in the chosen metric, then <span class="math inline">\(\boldsymbol{\theta}_s\)</span> is also the maximum a posteriori (MAP) estimator. By taking the negative logarithm and expanding in <span class="math inline">\(\boldsymbol{\theta}\)</span> around <span class="math inline">\(\boldsymbol{\theta}_s\)</span>, we may obtain the Fisher information matrix <span class="citation">[<a href="references.html#ref-fisher_1925" role="doc-biblioref">107</a>]</span> for the Asimov likelihood: <span id="eq:fisher_info"><span class="math display">\[
                {\boldsymbol{I}(\boldsymbol{\theta})}_{ij}
                = \frac{\partial^2}{\partial {\theta_i} \partial {\theta_j}}
                 \left ( - \log \mathcal{\hat{L}}_A(\boldsymbol{\theta};
                 \boldsymbol{\phi}) \right )
                \qquad(6.8)\]</span></span> which can be computed via automatic differentiation if the simulation is differentiable and included in the computation graph, or if the effect of varying <span class="math inline">\(\boldsymbol{\theta}\)</span> over the simulated dataset <span class="math inline">\(G_s\)</span> can be effectively approximated. While this requirement does constrain the applicability of the proposed technique to a subset of likelihood-free inference problems, it is quite common in e.g. physical sciences that the effect of the parameters of interest and the main nuisance parameters over a sample can be approximated by the changes of mixture coefficients of mixture models, translations of a subset of features, or conditional density ratio re-weighting.</p>
                <p>If <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is an unbiased estimator of the values of <span class="math inline">\(\boldsymbol{\theta}\)</span>, the covariance matrix fulfils the Cramér-Rao lower bound <span class="citation">[<a href="references.html#ref-cramer2016mathematical" role="doc-biblioref">108</a>], [<a href="references.html#ref-rao1992information" role="doc-biblioref">109</a>]</span>: <span id="eq:CRB"><span class="math display">\[
                \textrm{cov}_{\boldsymbol{\theta}}(\hat{\boldsymbol{\theta}}) \geq
                I(\boldsymbol{\theta})^{-1}
                \qquad(6.9)\]</span></span> and the inverse of the Fisher information can be used as an approximate estimator of the expected variance, given that the bound would become an equality in the asymptotic limit for MLE. If some of the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> are constrained by independent measurements characterised by their likelihoods <span class="math inline">\(\{\mathcal{L}_C^{0}(\boldsymbol{\theta}), ..., \mathcal{L}_{C}^{c}(\boldsymbol{\theta})\}\)</span>, those constraints can also be easily included in the covariance estimation, simply by considering the augmented likelihood <span class="math inline">\(\hat{\mathcal{L}}_A'\)</span> instead of <span class="math inline">\(\hat{\mathcal{L}}_A\)</span> in Eq. <a href="6-3-method.html#eq:fisher_info">6.8</a>: <span id="eq:add_constraint"><span class="math display">\[\hat{\mathcal{L}}_A'(\boldsymbol{\theta} ; \boldsymbol{\phi}) =
                \hat{\mathcal{L}}_A(\boldsymbol{\theta} ; \boldsymbol{\phi})
                \prod_{i=0}^{c}\mathcal{L}_C^i(\boldsymbol{\theta}).\qquad(6.10)\]</span></span> In Bayesian terminology, this approach is referred to as the Laplace approximation <span class="citation">[<a href="references.html#ref-laplace1986memoir" role="doc-biblioref">110</a>]</span> where the logarithm of the joint density (including the priors) is expanded around the MAP to a multi-dimensional normal approximation of the posterior density: <span id="eq:normal_approx"><span class="math display">\[
                p(\boldsymbol{\theta}|D) \approx \textrm{Normal}(
                \boldsymbol{\theta} ; \hat{\boldsymbol{\theta}},
                I(\hat{\boldsymbol{\theta})}^{-1} )
                \qquad(6.11)\]</span></span> which has already been approached by automatic differentiation in probabilistic programming frameworks <span class="citation">[<a href="references.html#ref-tran2016edward" role="doc-biblioref">192</a>]</span>. While a histogram has been used to construct a Poisson count sample likelihood, non-parametric density estimation techniques can be used in its place to construct a product of observation likelihoods based on the neural network output <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x}; \boldsymbol{\phi})\)</span> instead. For example, an extension of this technique to use kernel density estimation (KDE) should be straightforward, given its intrinsic differentiability.</p>
                <p>The loss function used for stochastic optimisation of the neural network parameters <span class="math inline">\(\boldsymbol{\phi}\)</span> can be any function of the inverse of the Fisher information matrix at <span class="math inline">\(\boldsymbol{\theta}_s\)</span>, depending on the ultimate inference aim. The diagonal elements <span class="math inline">\(I_{ii}^{-1}(\boldsymbol{\theta}_s)\)</span> correspond to the expected variance of each of the <span class="math inline">\(\phi_i\)</span> under the normal approximation mentioned before, so if the aim is efficient inference about one of the parameters <span class="math inline">\(\omega_0 = \theta_k\)</span> a candidate loss function is: <span id="eq:example_loss"><span class="math display">\[
                U = I_{kk}^{-1}(\boldsymbol{\theta}_s)
                \qquad(6.12)\]</span></span> which corresponds to the expected width of the confidence interval for <span class="math inline">\(\omega_0\)</span> accounting also for the effect of the other nuisance parameters in <span class="math inline">\(\boldsymbol{\theta}\)</span>. This approach can also be extended when the goal is inference over several parameters of interest <span class="math inline">\(\boldsymbol{\omega} \subseteq \boldsymbol{\theta}\)</span> (e.g. when considering a weighted sum of the relevant variances). A simple version of the approach just described to learn a neural-network based summary statistic employing an inference-aware loss is summarised in Algorithm <a href="6-3-method.html#alg:simple_algorithm">1</a>.</p>
                
                <div class="caption">
                <p>Algorithm 1: Inference-Aware Neural Optimisation.</p>
                </div>
                <p><img src="gfx/algos/simple_algorithm.svg" id="alg:simple_algorithm"></p>
                </div>
                </section></div>
          </div>
        </div>
        
        
      <a href="6-4-related-work.html" class="navigation navigation-next" aria-label="Next page">
                <i class="fa fa-angle-right"></i></a><a href="6-2-problem-statement.html" class="navigation navigation-prev" aria-label="Previous page">
                <i class="fa fa-angle-left"></i></a></div>
    </div>
    

    <script src="libs/gitbook/js/app.min.js"></script><script src="libs/gitbook/js/lunr.js"></script><script src="libs/gitbook/js/plugin-search.js"></script><script src="libs/gitbook/js/plugin-sharing.js"></script><script src="libs/gitbook/js/plugin-fontsettings.js"></script><script src="libs/gitbook/js/plugin-bookdown.js"></script><script src="libs/gitbook/js/jquery.highlight.js"></script><script>
      gitbook.require(["gitbook"], function(gitbook) {
        gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook","twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"history": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"toc": {
"collapse": "none"
}
});
});
    </script><script>
      (function () {
        var script = document.createElement("script");
        script.type = "text/javascript";
        var src = "true";
        if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
        if (location.protocol !== "file:" && /^https?:/.test(src))
          src = src.replace(/^https?:/, '');
        script.src = src;
        document.getElementsByTagName("head")[0].appendChild(script);
      })();
    </script><script src="https://hypothes.is/embed.js" async></script><link href="css/annotator.css" rel="stylesheet"></body></html>